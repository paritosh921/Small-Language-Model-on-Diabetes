{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTp6_ge_BaHE",
        "outputId": "da0dd2ff-673b-45a4-b652-42d6e5d11d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Dataset/Diabetes.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "NNceDsdmCYIZ"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0xK0lSnCYLZ",
        "outputId": "44dce811-d216-485e-b9ef-71e25bccc7bb"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of dataset in characters:  67053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjwR7Y10CYOT",
        "outputId": "e9826cfc-43c6-46d5-af5f-3f65fc2d20bf"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The History of Diabetes Mellitus\n",
            "Robert B. Tattersall\n",
            " University of Nottingham, Nottingham, UK\n",
            "Textbook of Diabetes, 4th edition. Edited by R. Holt, C. Cockram,\n",
            "A. Flyvbjerg and B. Goldstein. © 2010 Blackwell Publishing.\n",
            "Keypoints\n",
            "• Polyuric diseases have been described for over 3500 years. The name\n",
            " “ diabetes ” comes from the Greek word for a syphon; the sweet taste\n",
            "of diabetic urine was recognized at the beginning of the fi rst\n",
            "millennium, but the adjective “ mellitus ” (honeyed) was only added by\n",
            "Rollo in the late 18th century.\n",
            "• The sugar in diabetic urine was identifi ed as glucose by Chevreul in\n",
            "1815. In the 1840s, Bernard showed that glucose was normally present\n",
            "in blood, and showed that it was stored in the liver (as glycogen) for\n",
            "secretion into the bloodstream during fasting.\n",
            "• In 1889, Minkowski and von Mering reported that pancreatectomy\n",
            "caused severe diabetes in the dog. In 1893, Laguesse suggested that\n",
            "the pancreatic “ islets ” described by Langerhans in 1869 produced an\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "SA5ecbQdCYRZ"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "We1Hjfm8CYUd"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts([text])"
      ],
      "metadata": {
        "id": "luLcQw0CCYXc"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93wWSKrpCYae",
        "outputId": "11f58942-9a30-48e1-d3e9-9b3f67b8fa4e"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'of': 2,\n",
              " 'and': 3,\n",
              " 'in': 4,\n",
              " 'diabetes': 5,\n",
              " 'to': 6,\n",
              " 'a': 7,\n",
              " 'by': 8,\n",
              " 'that': 9,\n",
              " 'was': 10,\n",
              " 'with': 11,\n",
              " 'as': 12,\n",
              " 'insulin': 13,\n",
              " '–': 14,\n",
              " 'for': 15,\n",
              " 'is': 16,\n",
              " 'are': 17,\n",
              " 'from': 18,\n",
              " 'costs': 19,\n",
              " 'fi': 20,\n",
              " 'or': 21,\n",
              " '“': 22,\n",
              " '”': 23,\n",
              " '1': 24,\n",
              " 'it': 25,\n",
              " 'were': 26,\n",
              " 'this': 27,\n",
              " 'be': 28,\n",
              " 'health': 29,\n",
              " 'disease': 30,\n",
              " 'have': 31,\n",
              " 'these': 32,\n",
              " 'but': 33,\n",
              " 'on': 34,\n",
              " 'glucose': 35,\n",
              " 'care': 36,\n",
              " 'mortality': 37,\n",
              " 'had': 38,\n",
              " 'people': 39,\n",
              " 'complications': 40,\n",
              " 'an': 41,\n",
              " 'may': 42,\n",
              " 'patients': 43,\n",
              " 's': 44,\n",
              " 'rst': 45,\n",
              " 'not': 46,\n",
              " 'more': 47,\n",
              " 'such': 48,\n",
              " '5': 49,\n",
              " 'blood': 50,\n",
              " 'risk': 51,\n",
              " 'c': 52,\n",
              " 'only': 53,\n",
              " 'type': 54,\n",
              " '2': 55,\n",
              " '’': 56,\n",
              " 'which': 57,\n",
              " 'us': 58,\n",
              " 'because': 59,\n",
              " 'associated': 60,\n",
              " 'diabetic': 61,\n",
              " 'also': 62,\n",
              " 'cells': 63,\n",
              " 'been': 64,\n",
              " 'described': 65,\n",
              " 'who': 66,\n",
              " 'they': 67,\n",
              " 'related': 68,\n",
              " 'greater': 69,\n",
              " 'into': 70,\n",
              " 'developed': 71,\n",
              " 'all': 72,\n",
              " 'at': 73,\n",
              " 'islet': 74,\n",
              " 'one': 75,\n",
              " '4': 76,\n",
              " 'years': 77,\n",
              " 'figure': 78,\n",
              " 'than': 79,\n",
              " 'e': 80,\n",
              " 'cost': 81,\n",
              " 'urine': 82,\n",
              " 'control': 83,\n",
              " 'well': 84,\n",
              " 'there': 85,\n",
              " 'countries': 86,\n",
              " '•': 87,\n",
              " 'islets': 88,\n",
              " 'pancreas': 89,\n",
              " 'used': 90,\n",
              " 'life': 91,\n",
              " 'while': 92,\n",
              " 'between': 93,\n",
              " '6': 94,\n",
              " 'some': 95,\n",
              " 'most': 96,\n",
              " 'medical': 97,\n",
              " 'studies': 98,\n",
              " 'age': 99,\n",
              " 't2dm': 100,\n",
              " 'disability': 101,\n",
              " 'b': 102,\n",
              " 'reported': 103,\n",
              " 'treatment': 104,\n",
              " 'clinical': 105,\n",
              " 'cell': 106,\n",
              " 'onset': 107,\n",
              " 'work': 108,\n",
              " 'year': 109,\n",
              " 'usa': 110,\n",
              " 'g': 111,\n",
              " 'during': 112,\n",
              " 'pancreatic': 113,\n",
              " 'use': 114,\n",
              " 'when': 115,\n",
              " 'its': 116,\n",
              " 'those': 117,\n",
              " 'renal': 118,\n",
              " 'has': 119,\n",
              " 'resource': 120,\n",
              " 'β': 121,\n",
              " 'young': 122,\n",
              " 'early': 123,\n",
              " 'both': 124,\n",
              " 'their': 125,\n",
              " 'after': 126,\n",
              " 'other': 127,\n",
              " 'signifi': 128,\n",
              " '10': 129,\n",
              " 'result': 130,\n",
              " 'can': 131,\n",
              " '3': 132,\n",
              " 'acute': 133,\n",
              " 'deaths': 134,\n",
              " 'vascular': 135,\n",
              " 'without': 136,\n",
              " 'will': 137,\n",
              " 'economic': 138,\n",
              " 'increases': 139,\n",
              " 'cvd': 140,\n",
              " 'income': 141,\n",
              " 'data': 142,\n",
              " 'sugar': 143,\n",
              " 'given': 144,\n",
              " 'could': 145,\n",
              " 'he': 146,\n",
              " 'higher': 147,\n",
              " 'general': 148,\n",
              " 'new': 149,\n",
              " 'illness': 150,\n",
              " 'no': 151,\n",
              " 'therapeutic': 152,\n",
              " '75': 153,\n",
              " 'burdens': 154,\n",
              " 'worldwide': 155,\n",
              " 'settings': 156,\n",
              " 'lmics': 157,\n",
              " 'global': 158,\n",
              " 'direct': 159,\n",
              " 'over': 160,\n",
              " 'ed': 161,\n",
              " 'secretion': 162,\n",
              " 'caused': 163,\n",
              " 'severe': 164,\n",
              " 'discovered': 165,\n",
              " 'macleod': 166,\n",
              " 'human': 167,\n",
              " 'study': 168,\n",
              " 'low': 169,\n",
              " 'would': 170,\n",
              " 'fl': 171,\n",
              " 'his': 172,\n",
              " 'being': 173,\n",
              " 'up': 174,\n",
              " 'development': 175,\n",
              " 'made': 176,\n",
              " 'even': 177,\n",
              " 'i': 178,\n",
              " 'long': 179,\n",
              " '7': 180,\n",
              " 'further': 181,\n",
              " 'injection': 182,\n",
              " '11': 183,\n",
              " 'management': 184,\n",
              " 'where': 185,\n",
              " 'high': 186,\n",
              " '40': 187,\n",
              " 'drugs': 188,\n",
              " 'morbidity': 189,\n",
              " 'individual': 190,\n",
              " 'poor': 191,\n",
              " 'estimates': 192,\n",
              " 'expenditure': 193,\n",
              " 'population': 194,\n",
              " 'less': 195,\n",
              " 'indirect': 196,\n",
              " 'diseases': 197,\n",
              " 'century': 198,\n",
              " 'showed': 199,\n",
              " 'metabolism': 200,\n",
              " 'banting': 201,\n",
              " 'best': 202,\n",
              " 'action': 203,\n",
              " 'preparations': 204,\n",
              " 'include': 205,\n",
              " 't': 206,\n",
              " 'term': 207,\n",
              " 'time': 208,\n",
              " 'diagnosis': 209,\n",
              " 'led': 210,\n",
              " 'found': 211,\n",
              " 'causes': 212,\n",
              " 'extract': 213,\n",
              " 'lowering': 214,\n",
              " 'two': 215,\n",
              " 'cases': 216,\n",
              " 'increased': 217,\n",
              " 'utilization': 218,\n",
              " 'world': 219,\n",
              " 'glycemic': 220,\n",
              " 'major': 221,\n",
              " '67': 222,\n",
              " 'burden': 223,\n",
              " 'proportion': 224,\n",
              " 't1dm': 225,\n",
              " 'approximately': 226,\n",
              " 'estimated': 227,\n",
              " 'although': 228,\n",
              " 'regions': 229,\n",
              " 'productivity': 230,\n",
              " 'suggested': 231,\n",
              " 'langerhans': 232,\n",
              " 'produced': 233,\n",
              " 'internal': 234,\n",
              " 'è': 235,\n",
              " 'non': 236,\n",
              " 'failure': 237,\n",
              " 'retinopathy': 238,\n",
              " 'half': 239,\n",
              " 'common': 240,\n",
              " 'specifi': 241,\n",
              " 'followed': 242,\n",
              " 'nephropathy': 243,\n",
              " 'table': 244,\n",
              " 'large': 245,\n",
              " 'them': 246,\n",
              " 'either': 247,\n",
              " 'within': 248,\n",
              " 'then': 249,\n",
              " 'discovery': 250,\n",
              " 'period': 251,\n",
              " 'infl': 252,\n",
              " 'evidence': 253,\n",
              " 'cause': 254,\n",
              " '8': 255,\n",
              " 'role': 256,\n",
              " 'through': 257,\n",
              " 'about': 258,\n",
              " '12': 259,\n",
              " 'least': 260,\n",
              " 'hypoglycemia': 261,\n",
              " 'prevent': 262,\n",
              " 'using': 263,\n",
              " 'addition': 264,\n",
              " 'different': 265,\n",
              " 'chronic': 266,\n",
              " 'd': 267,\n",
              " 'groups': 268,\n",
              " 'hypertension': 269,\n",
              " '15': 270,\n",
              " 'patient': 271,\n",
              " '65': 272,\n",
              " 'metabolic': 273,\n",
              " 'death': 274,\n",
              " 'lost': 275,\n",
              " 'co': 276,\n",
              " 'complex': 277,\n",
              " 'africa': 278,\n",
              " 'factors': 279,\n",
              " 'increase': 280,\n",
              " 'dysfunction': 281,\n",
              " 'lower': 282,\n",
              " 'consultations': 283,\n",
              " 'activities': 284,\n",
              " 'prevalence': 285,\n",
              " 'history': 286,\n",
              " 'bernard': 287,\n",
              " 'liver': 288,\n",
              " '1889': 289,\n",
              " 'minkowski': 290,\n",
              " '1869': 291,\n",
              " '1922': 292,\n",
              " 'subjects': 293,\n",
              " 'many': 294,\n",
              " 'maturity': 295,\n",
              " 'various': 296,\n",
              " 'symptoms': 297,\n",
              " 'kimmelstiel': 298,\n",
              " 'wilson': 299,\n",
              " 'concept': 300,\n",
              " 'introduced': 301,\n",
              " 'progression': 302,\n",
              " 'account': 303,\n",
              " 'very': 304,\n",
              " 'known': 305,\n",
              " 'much': 306,\n",
              " 'important': 307,\n",
              " 'any': 308,\n",
              " 'function': 309,\n",
              " 'next': 310,\n",
              " 'effects': 311,\n",
              " 'limited': 312,\n",
              " 'loss': 313,\n",
              " 'therapy': 314,\n",
              " 'ned': 315,\n",
              " 'published': 316,\n",
              " '27': 317,\n",
              " 'defi': 318,\n",
              " 'rapidly': 319,\n",
              " 'example': 320,\n",
              " 'individuals': 321,\n",
              " 'permanent': 322,\n",
              " 'cant': 323,\n",
              " 'number': 324,\n",
              " 'methods': 325,\n",
              " 'oral': 326,\n",
              " 'compared': 327,\n",
              " 'group': 328,\n",
              " 'impacts': 329,\n",
              " 'social': 330,\n",
              " 'national': 331,\n",
              " 'morbidities': 332,\n",
              " 'increasing': 333,\n",
              " 'duration': 334,\n",
              " 'respectively': 335,\n",
              " 'germany': 336,\n",
              " 'form': 337,\n",
              " 'india': 338,\n",
              " 'total': 339,\n",
              " 'times': 340,\n",
              " 'organization': 341,\n",
              " 'attributable': 342,\n",
              " 'losses': 343,\n",
              " 'osts': 344,\n",
              " 'bn': 345,\n",
              " 'per': 346,\n",
              " 'expressing': 347,\n",
              " 'α': 348,\n",
              " 'mellitus': 349,\n",
              " 'r': 350,\n",
              " 'recognized': 351,\n",
              " 'late': 352,\n",
              " 'present': 353,\n",
              " 'dog': 354,\n",
              " '1921': 355,\n",
              " 'collip': 356,\n",
              " 'extracts': 357,\n",
              " 'diab': 358,\n",
              " 'te': 359,\n",
              " '1930s': 360,\n",
              " 'types': 361,\n",
              " 'dependent': 362,\n",
              " 'resistance': 363,\n",
              " 'infi': 364,\n",
              " 'presence': 365,\n",
              " 'benefi': 366,\n",
              " 'introduction': 367,\n",
              " 'does': 368,\n",
              " 'ow': 369,\n",
              " 'among': 370,\n",
              " 'short': 371,\n",
              " 'become': 372,\n",
              " 'so': 373,\n",
              " 'research': 374,\n",
              " 'ability': 375,\n",
              " 'measure': 376,\n",
              " 'theory': 377,\n",
              " 'nding': 378,\n",
              " 'indeed': 379,\n",
              " 'j': 380,\n",
              " 'drivers': 381,\n",
              " 'did': 382,\n",
              " 'possible': 383,\n",
              " 'controlled': 384,\n",
              " 'injections': 385,\n",
              " 'thyroid': 386,\n",
              " '22': 387,\n",
              " 'paul': 388,\n",
              " 'having': 389,\n",
              " 'out': 390,\n",
              " 'latin': 391,\n",
              " 'fact': 392,\n",
              " 'widely': 393,\n",
              " '13': 394,\n",
              " 'glycosuria': 395,\n",
              " '23': 396,\n",
              " 'toronto': 397,\n",
              " 'later': 398,\n",
              " 'events': 399,\n",
              " 'according': 400,\n",
              " 'active': 401,\n",
              " 'results': 402,\n",
              " 'values': 403,\n",
              " 'nite': 404,\n",
              " 'america': 405,\n",
              " 'do': 406,\n",
              " 'day': 407,\n",
              " 'often': 408,\n",
              " 'serious': 409,\n",
              " '50': 410,\n",
              " 'required': 411,\n",
              " 'et': 412,\n",
              " 'al': 413,\n",
              " 'until': 414,\n",
              " '80': 415,\n",
              " 'syndrome': 416,\n",
              " 'denmark': 417,\n",
              " 'particularly': 418,\n",
              " '20': 419,\n",
              " 'hospital': 420,\n",
              " 'cardiovascular': 421,\n",
              " 'average': 422,\n",
              " 'daily': 423,\n",
              " '69': 424,\n",
              " 'better': 425,\n",
              " 'monitoring': 426,\n",
              " 'however': 427,\n",
              " 'each': 428,\n",
              " 'resources': 429,\n",
              " 'value': 430,\n",
              " 'patterns': 431,\n",
              " 'vary': 432,\n",
              " 'accounts': 433,\n",
              " 'consequences': 434,\n",
              " 'underlying': 435,\n",
              " 'episodes': 436,\n",
              " 'dka': 437,\n",
              " 'premature': 438,\n",
              " 'chd': 439,\n",
              " '60': 440,\n",
              " 'implications': 441,\n",
              " 'seeking': 442,\n",
              " 'preventative': 443,\n",
              " '74': 444,\n",
              " 'annually': 445,\n",
              " 'hospitalization': 446,\n",
              " 'suggest': 447,\n",
              " 'expenses': 448,\n",
              " 'diffi': 449,\n",
              " 'medications': 450,\n",
              " 'society': 451,\n",
              " 'psychosocial': 452,\n",
              " 'physical': 453,\n",
              " 'depending': 454,\n",
              " 'country': 455,\n",
              " 'affect': 456,\n",
              " 'especially': 457,\n",
              " 'relative': 458,\n",
              " 'self': 459,\n",
              " 'diagnostic': 460,\n",
              " 'future': 461,\n",
              " 'pproach': 462,\n",
              " 'escalate': 463,\n",
              " 'annual': 464,\n",
              " 'populations': 465,\n",
              " '∼': 466,\n",
              " 'pre': 467,\n",
              " 'δ': 468,\n",
              " 'tattersall': 469,\n",
              " 'uk': 470,\n",
              " '2010': 471,\n",
              " 'word': 472,\n",
              " 'identifi': 473,\n",
              " 'glycogen': 474,\n",
              " 'von': 475,\n",
              " 'pancreatectomy': 476,\n",
              " 'laguesse': 477,\n",
              " 'january': 478,\n",
              " 'obese': 479,\n",
              " 'defronzo': 480,\n",
              " 'insulitis': 481,\n",
              " '1901': 482,\n",
              " '1979': 483,\n",
              " 'sequence': 484,\n",
              " '1955': 485,\n",
              " 'three': 486,\n",
              " '19th': 487,\n",
              " 'neuropathy': 488,\n",
              " 'albuminuria': 489,\n",
              " 'noted': 490,\n",
              " 'kidney': 491,\n",
              " '1936': 492,\n",
              " 'delayed': 493,\n",
              " 'tolbutamide': 494,\n",
              " '1960': 495,\n",
              " 'phenformin': 496,\n",
              " 'became': 497,\n",
              " 'available': 498,\n",
              " '1959': 499,\n",
              " 'metformin': 500,\n",
              " 'proved': 501,\n",
              " 'trial': 502,\n",
              " 'prospective': 503,\n",
              " 'meyer': 504,\n",
              " 'importance': 505,\n",
              " 'demonstrated': 506,\n",
              " 'ketoacidosis': 507,\n",
              " '1970s': 508,\n",
              " 'what': 509,\n",
              " 'now': 510,\n",
              " 'called': 511,\n",
              " 'down': 512,\n",
              " 'making': 513,\n",
              " 'like': 514,\n",
              " 'drinking': 515,\n",
              " 'if': 516,\n",
              " 'affected': 517,\n",
              " 'middle': 518,\n",
              " 'agents': 519,\n",
              " 'physiology': 520,\n",
              " 'plants': 521,\n",
              " 'states': 522,\n",
              " 'normal': 523,\n",
              " 'another': 524,\n",
              " 'great': 525,\n",
              " 'temporary': 526,\n",
              " 'hyperglycemia': 527,\n",
              " 'involved': 528,\n",
              " 'usually': 529,\n",
              " 'show': 530,\n",
              " 'came': 531,\n",
              " 'previously': 532,\n",
              " 'carbohydrate': 533,\n",
              " 'tissue': 534,\n",
              " 'still': 535,\n",
              " 'measured': 536,\n",
              " '25': 537,\n",
              " 'how': 538,\n",
              " 'whether': 539,\n",
              " 'since': 540,\n",
              " 'book': 541,\n",
              " 'approach': 542,\n",
              " '14': 543,\n",
              " '30': 544,\n",
              " 'level': 545,\n",
              " 'days': 546,\n",
              " 'concluded': 547,\n",
              " 'reduced': 548,\n",
              " 'following': 549,\n",
              " 'equivalent': 550,\n",
              " 'end': 551,\n",
              " '1923': 552,\n",
              " 'throughout': 553,\n",
              " 'recognition': 554,\n",
              " 'same': 555,\n",
              " 'p': 556,\n",
              " 'once': 557,\n",
              " 'fatal': 558,\n",
              " 'before': 559,\n",
              " '1950': 560,\n",
              " 'strategies': 561,\n",
              " 'rest': 562,\n",
              " 'observations': 563,\n",
              " 'latter': 564,\n",
              " '31': 565,\n",
              " 'hypoglycemic': 566,\n",
              " 'including': 567,\n",
              " '32': 568,\n",
              " 'profound': 569,\n",
              " 'remained': 570,\n",
              " 'colleagues': 571,\n",
              " 'destruction': 572,\n",
              " '34': 573,\n",
              " 'endocrine': 574,\n",
              " 'ica': 575,\n",
              " 'together': 576,\n",
              " 'transplantation': 577,\n",
              " 'hope': 578,\n",
              " 'signs': 579,\n",
              " 'alone': 580,\n",
              " 'heavy': 581,\n",
              " 'considerable': 582,\n",
              " 'ndings': 583,\n",
              " '43': 584,\n",
              " 'occurred': 585,\n",
              " 'elderly': 586,\n",
              " 'york': 587,\n",
              " 'numbers': 588,\n",
              " 'despite': 589,\n",
              " 'measures': 590,\n",
              " 'four': 591,\n",
              " 'regular': 592,\n",
              " 'protamine': 593,\n",
              " 'acting': 594,\n",
              " 'insulins': 595,\n",
              " 'specialists': 596,\n",
              " 'variety': 597,\n",
              " 'produce': 598,\n",
              " 'highly': 599,\n",
              " 'devices': 600,\n",
              " 'marketed': 601,\n",
              " 'withdrawn': 602,\n",
              " 'lack': 603,\n",
              " '70': 604,\n",
              " 'france': 605,\n",
              " '72': 606,\n",
              " 'system': 607,\n",
              " 'free': 608,\n",
              " 'linked': 609,\n",
              " '76': 610,\n",
              " 'should': 611,\n",
              " '90': 612,\n",
              " '95': 613,\n",
              " 'hyperglycemic': 614,\n",
              " 'follow': 615,\n",
              " 'infection': 616,\n",
              " 'african': 617,\n",
              " 'occur': 618,\n",
              " '0': 619,\n",
              " 'additional': 620,\n",
              " 'outcomes': 621,\n",
              " 'accessibility': 622,\n",
              " 'inadequate': 623,\n",
              " 'expectancy': 624,\n",
              " 'macrovascular': 625,\n",
              " 'heart': 626,\n",
              " 'stroke': 627,\n",
              " 'peripheral': 628,\n",
              " 'microvascular': 629,\n",
              " 'quality': 630,\n",
              " 'central': 631,\n",
              " 'mediating': 632,\n",
              " 'factor': 633,\n",
              " '53': 634,\n",
              " 'developing': 635,\n",
              " 'excess': 636,\n",
              " 'cumulatively': 637,\n",
              " 'therefore': 638,\n",
              " 'esrd': 639,\n",
              " 'requiring': 640,\n",
              " 'case': 641,\n",
              " 'circulation': 642,\n",
              " 'susceptibility': 643,\n",
              " 'amputation': 644,\n",
              " 'awareness': 645,\n",
              " 'ill': 646,\n",
              " 'service': 647,\n",
              " 'visits': 648,\n",
              " 'outpatient': 649,\n",
              " 'million': 650,\n",
              " 'shown': 651,\n",
              " 'leading': 652,\n",
              " 'almost': 653,\n",
              " '85': 654,\n",
              " 'household': 655,\n",
              " '84': 656,\n",
              " 'functioning': 657,\n",
              " 'es': 658,\n",
              " 'societal': 659,\n",
              " 'roles': 660,\n",
              " 'employed': 661,\n",
              " 'education': 662,\n",
              " 'occurrence': 663,\n",
              " 'illnesses': 664,\n",
              " 'economically': 665,\n",
              " 'widespread': 666,\n",
              " 'scarcity': 667,\n",
              " 'requires': 668,\n",
              " 'costly': 669,\n",
              " 'towards': 670,\n",
              " 'regarding': 671,\n",
              " 'estimating': 672,\n",
              " 'paramedical': 673,\n",
              " 'home': 674,\n",
              " 'foregone': 675,\n",
              " 'family': 676,\n",
              " 'm': 677,\n",
              " 'f': 678,\n",
              " 'disparities': 679,\n",
              " 'billion': 680,\n",
              " 'differences': 681,\n",
              " 'relevant': 682,\n",
              " '116': 683,\n",
              " 'expenditures': 684,\n",
              " 'urban': 685,\n",
              " 'reliable': 686,\n",
              " 'supply': 687,\n",
              " 'polypeptide': 688,\n",
              " 'neuropeptides': 689,\n",
              " 'robert': 690,\n",
              " 'university': 691,\n",
              " 'nottingham': 692,\n",
              " '4th': 693,\n",
              " 'edition': 694,\n",
              " 'edited': 695,\n",
              " 'holt': 696,\n",
              " 'cockram': 697,\n",
              " 'flyvbjerg': 698,\n",
              " 'goldstein': 699,\n",
              " '©': 700,\n",
              " 'blackwell': 701,\n",
              " 'publishing': 702,\n",
              " 'polyuric': 703,\n",
              " 'greek': 704,\n",
              " 'syphon': 705,\n",
              " 'added': 706,\n",
              " 'chevreul': 707,\n",
              " '1815': 708,\n",
              " 'normally': 709,\n",
              " 'fasting': 710,\n",
              " 'mering': 711,\n",
              " '1893': 712,\n",
              " 'acid': 713,\n",
              " 'ethanol': 714,\n",
              " 'maigre': 715,\n",
              " 'gras': 716,\n",
              " 'lancereaux': 717,\n",
              " 'falta': 718,\n",
              " 'himsworth': 719,\n",
              " 'sensitive': 720,\n",
              " 'insensitive': 721,\n",
              " 'classifi': 722,\n",
              " 'defects': 723,\n",
              " 'investigated': 724,\n",
              " 'clamp': 725,\n",
              " 'method': 726,\n",
              " 'devised': 727,\n",
              " 'accurate': 728,\n",
              " 'technique': 729,\n",
              " 'distinct': 730,\n",
              " '1974': 731,\n",
              " 'lymphocytic': 732,\n",
              " 'ltration': 733,\n",
              " 'highlighted': 734,\n",
              " '1965': 735,\n",
              " 'gepts': 736,\n",
              " 'might': 737,\n",
              " 'antibodies': 738,\n",
              " 'doniach': 739,\n",
              " 'bottazzo': 740,\n",
              " 'primary': 741,\n",
              " 'structure': 742,\n",
              " '1967': 743,\n",
              " 'invented': 744,\n",
              " '1971': 745,\n",
              " 'protein': 746,\n",
              " 'second': 747,\n",
              " 'unique': 748,\n",
              " 'angiopathy': 749,\n",
              " 'milestones': 750,\n",
              " 'included': 751,\n",
              " '1940s': 752,\n",
              " 'synthetic': 753,\n",
              " 'analogs': 754,\n",
              " 'recombinant': 755,\n",
              " 'dna': 756,\n",
              " 'technology': 757,\n",
              " 'sulfonylurea': 758,\n",
              " 'carbutamide': 759,\n",
              " '1957': 760,\n",
              " 'biguanide': 761,\n",
              " '1993': 762,\n",
              " 'landmarks': 763,\n",
              " 'pressure': 764,\n",
              " 'dose': 765,\n",
              " 'features': 766,\n",
              " 'georg': 767,\n",
              " 'aretaeus': 768,\n",
              " 'remain': 769,\n",
              " 'body': 770,\n",
              " 'whereby': 771,\n",
              " 'leave': 772,\n",
              " 'incessant': 773,\n",
              " 'men': 774,\n",
              " 'stop': 775,\n",
              " 'water': 776,\n",
              " 'thirst': 777,\n",
              " 'bodies': 778,\n",
              " 'tests': 779,\n",
              " 'reducing': 780,\n",
              " '1848': 781,\n",
              " 'measurement': 782,\n",
              " 'done': 783,\n",
              " 'needed': 784,\n",
              " '1913': 785,\n",
              " 'physician': 786,\n",
              " 'christian': 787,\n",
              " 'whose': 788,\n",
              " 'place': 789,\n",
              " 'began': 790,\n",
              " 'prevailing': 791,\n",
              " 'originally': 792,\n",
              " 'thought': 793,\n",
              " 'meals': 794,\n",
              " 'animals': 795,\n",
              " 'starch': 796,\n",
              " 'converted': 797,\n",
              " 'regarded': 798,\n",
              " 'analogous': 799,\n",
              " 'hypothesis': 800,\n",
              " 'impression': 801,\n",
              " 'nervous': 802,\n",
              " 'bodily': 803,\n",
              " 'scientifi': 804,\n",
              " 'uences': 805,\n",
              " 'mystery': 806,\n",
              " 'autopsy': 807,\n",
              " 'investigating': 808,\n",
              " 'said': 809,\n",
              " 'laboratory': 810,\n",
              " 'mentioned': 811,\n",
              " 'trained': 812,\n",
              " 'cance': 813,\n",
              " 'explanations': 814,\n",
              " 'charles': 815,\n",
              " 'é': 816,\n",
              " '9': 817,\n",
              " 'myxoedema': 818,\n",
              " 'orally': 819,\n",
              " 'islands': 820,\n",
              " 'old': 821,\n",
              " 'clusters': 822,\n",
              " 'named': 823,\n",
              " '1909': 824,\n",
              " 'belgian': 825,\n",
              " 'de': 826,\n",
              " 'insuline': 827,\n",
              " 'decades': 828,\n",
              " 'disorder': 829,\n",
              " 'vienna': 830,\n",
              " 'proposed': 831,\n",
              " 'attempts': 832,\n",
              " 'biologic': 833,\n",
              " 'activity': 834,\n",
              " '1949': 835,\n",
              " '24': 836,\n",
              " '1920': 837,\n",
              " 'antidiabetic': 838,\n",
              " 'extraction': 839,\n",
              " 'decided': 840,\n",
              " 'likely': 841,\n",
              " 'eventually': 842,\n",
              " 'scotland': 843,\n",
              " 'student': 844,\n",
              " 'chosen': 845,\n",
              " 'months': 846,\n",
              " 'excellent': 847,\n",
              " '26': 848,\n",
              " 'prepared': 849,\n",
              " 'obtained': 850,\n",
              " 'greatly': 851,\n",
              " 'morning': 852,\n",
              " 'marked': 853,\n",
              " 'improvement': 854,\n",
              " 'initial': 855,\n",
              " 'seven': 856,\n",
              " 'association': 857,\n",
              " 'condition': 858,\n",
              " 'report': 859,\n",
              " 'sense': 860,\n",
              " 'administration': 861,\n",
              " 'unaware': 862,\n",
              " 'earlier': 863,\n",
              " '28': 864,\n",
              " '19': 865,\n",
              " 'reports': 866,\n",
              " '1924': 867,\n",
              " 'europe': 868,\n",
              " 'international': 869,\n",
              " 'prize': 870,\n",
              " 'confi': 871,\n",
              " 'soon': 872,\n",
              " 'obvious': 873,\n",
              " 'xed': 874,\n",
              " 'injected': 875,\n",
              " 'varied': 876,\n",
              " 'ever': 877,\n",
              " 'saved': 878,\n",
              " 'died': 879,\n",
              " 'effect': 880,\n",
              " 'joslin': 881,\n",
              " 'dying': 882,\n",
              " 'today': 883,\n",
              " 'chapter': 884,\n",
              " 'h': 885,\n",
              " 'iabetes': 886,\n",
              " 'single': 887,\n",
              " 'helped': 888,\n",
              " '29': 889,\n",
              " '93': 890,\n",
              " 'london': 891,\n",
              " 'resistant': 892,\n",
              " 'older': 893,\n",
              " 'ketosis': 894,\n",
              " 'ciency': 895,\n",
              " 'l': 896,\n",
              " '33': 897,\n",
              " 'apparently': 898,\n",
              " 'autoimmune': 899,\n",
              " '1912': 900,\n",
              " '1946': 901,\n",
              " '35': 902,\n",
              " '1939': 903,\n",
              " 'lead': 904,\n",
              " 'raised': 905,\n",
              " 'drug': 906,\n",
              " 'doses': 907,\n",
              " '37': 908,\n",
              " '2005': 909,\n",
              " 'problem': 910,\n",
              " '2000': 911,\n",
              " 'omplications': 912,\n",
              " 'assumed': 913,\n",
              " 'papers': 914,\n",
              " 'retinal': 915,\n",
              " 'henry': 916,\n",
              " 'russell': 917,\n",
              " 'wilder': 918,\n",
              " 'mayo': 919,\n",
              " 'clinic': 920,\n",
              " 'must': 921,\n",
              " 'mean': 922,\n",
              " 'probably': 923,\n",
              " '1997': 924,\n",
              " 'nodules': 925,\n",
              " 'eight': 926,\n",
              " 'paper': 927,\n",
              " '1954': 928,\n",
              " '44': 929,\n",
              " 'standing': 930,\n",
              " 'objective': 931,\n",
              " 'particular': 932,\n",
              " '1947': 933,\n",
              " 'accepted': 934,\n",
              " 'criteria': 935,\n",
              " '63': 936,\n",
              " 'receive': 937,\n",
              " 'cohort': 938,\n",
              " '64': 939,\n",
              " 'third': 940,\n",
              " 'levels': 941,\n",
              " 'appeared': 942,\n",
              " 'soluble': 943,\n",
              " 'multiple': 944,\n",
              " 'preparation': 945,\n",
              " 'hagedorn': 946,\n",
              " '17': 947,\n",
              " 'initially': 948,\n",
              " 'main': 949,\n",
              " '66': 950,\n",
              " 'viewpoint': 951,\n",
              " 'replacement': 952,\n",
              " 'syringes': 953,\n",
              " 'ne': 954,\n",
              " 'john': 955,\n",
              " '68': 956,\n",
              " 'demand': 957,\n",
              " 'guanidine': 958,\n",
              " 'derivative': 959,\n",
              " 'frank': 960,\n",
              " 'recurrent': 961,\n",
              " 'loubati': 962,\n",
              " 'res': 963,\n",
              " 'effective': 964,\n",
              " '71': 965,\n",
              " 'practice': 966,\n",
              " 'market': 967,\n",
              " '1994': 968,\n",
              " 'class': 969,\n",
              " 'damage': 970,\n",
              " 'glucagon': 971,\n",
              " 'glp': 972,\n",
              " 'randomized': 973,\n",
              " '73': 974,\n",
              " 'criticized': 975,\n",
              " 'placebo': 976,\n",
              " 'suggesting': 977,\n",
              " 'advocated': 978,\n",
              " 'way': 979,\n",
              " 'testing': 980,\n",
              " 'tolstoi': 981,\n",
              " 'worth': 982,\n",
              " 'living': 983,\n",
              " 'able': 984,\n",
              " 'policy': 985,\n",
              " 'families': 986,\n",
              " 'economies': 987,\n",
              " 'perspective': 988,\n",
              " 'taken': 989,\n",
              " 'describe': 990,\n",
              " 'net': 991,\n",
              " 'rmity': 992,\n",
              " 'derived': 993,\n",
              " 'investment': 994,\n",
              " 'amount': 995,\n",
              " 'forms': 996,\n",
              " 'frequency': 997,\n",
              " 'varies': 998,\n",
              " 'pathophysiology': 999,\n",
              " 'serum': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSxtYBVHCYdp",
        "outputId": "13cf9d82-95d6-463e-b47b-a86cf9d684ab"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2777"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "for sentence in text.split('\\n'):\n",
        "  tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "  for i in range(1,len(tokenized_sentence)):\n",
        "    input_sequences.append(tokenized_sentence[:i+1])"
      ],
      "metadata": {
        "id": "pYWoYD1GCYhx"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGf3zrj5CYkH",
        "outputId": "c06cda6b-f003-48f7-b44b-b1ee721d5356"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 286],\n",
              " [1, 286, 2],\n",
              " [1, 286, 2, 5],\n",
              " [1, 286, 2, 5, 349],\n",
              " [690, 102],\n",
              " [690, 102, 469],\n",
              " [691, 2],\n",
              " [691, 2, 692],\n",
              " [691, 2, 692, 692],\n",
              " [691, 2, 692, 692, 470],\n",
              " [1193, 2],\n",
              " [1193, 2, 5],\n",
              " [1193, 2, 5, 693],\n",
              " [1193, 2, 5, 693, 694],\n",
              " [1193, 2, 5, 693, 694, 695],\n",
              " [1193, 2, 5, 693, 694, 695, 8],\n",
              " [1193, 2, 5, 693, 694, 695, 8, 350],\n",
              " [1193, 2, 5, 693, 694, 695, 8, 350, 696],\n",
              " [1193, 2, 5, 693, 694, 695, 8, 350, 696, 52],\n",
              " [1193, 2, 5, 693, 694, 695, 8, 350, 696, 52, 697],\n",
              " [7, 698],\n",
              " [7, 698, 3],\n",
              " [7, 698, 3, 102],\n",
              " [7, 698, 3, 102, 699],\n",
              " [7, 698, 3, 102, 699, 700],\n",
              " [7, 698, 3, 102, 699, 700, 471],\n",
              " [7, 698, 3, 102, 699, 700, 471, 701],\n",
              " [7, 698, 3, 102, 699, 700, 471, 701, 702],\n",
              " [87, 703],\n",
              " [87, 703, 197],\n",
              " [87, 703, 197, 31],\n",
              " [87, 703, 197, 31, 64],\n",
              " [87, 703, 197, 31, 64, 65],\n",
              " [87, 703, 197, 31, 64, 65, 15],\n",
              " [87, 703, 197, 31, 64, 65, 15, 160],\n",
              " [87, 703, 197, 31, 64, 65, 15, 160, 1195],\n",
              " [87, 703, 197, 31, 64, 65, 15, 160, 1195, 77],\n",
              " [87, 703, 197, 31, 64, 65, 15, 160, 1195, 77, 1],\n",
              " [87, 703, 197, 31, 64, 65, 15, 160, 1195, 77, 1, 1196],\n",
              " [22, 5],\n",
              " [22, 5, 23],\n",
              " [22, 5, 23, 1197],\n",
              " [22, 5, 23, 1197, 18],\n",
              " [22, 5, 23, 1197, 18, 1],\n",
              " [22, 5, 23, 1197, 18, 1, 704],\n",
              " [22, 5, 23, 1197, 18, 1, 704, 472],\n",
              " [22, 5, 23, 1197, 18, 1, 704, 472, 15],\n",
              " [22, 5, 23, 1197, 18, 1, 704, 472, 15, 7],\n",
              " [22, 5, 23, 1197, 18, 1, 704, 472, 15, 7, 705],\n",
              " [22, 5, 23, 1197, 18, 1, 704, 472, 15, 7, 705, 1],\n",
              " [22, 5, 23, 1197, 18, 1, 704, 472, 15, 7, 705, 1, 1198],\n",
              " [22, 5, 23, 1197, 18, 1, 704, 472, 15, 7, 705, 1, 1198, 1199],\n",
              " [2, 61],\n",
              " [2, 61, 82],\n",
              " [2, 61, 82, 10],\n",
              " [2, 61, 82, 10, 351],\n",
              " [2, 61, 82, 10, 351, 73],\n",
              " [2, 61, 82, 10, 351, 73, 1],\n",
              " [2, 61, 82, 10, 351, 73, 1, 1200],\n",
              " [2, 61, 82, 10, 351, 73, 1, 1200, 2],\n",
              " [2, 61, 82, 10, 351, 73, 1, 1200, 2, 1],\n",
              " [2, 61, 82, 10, 351, 73, 1, 1200, 2, 1, 20],\n",
              " [2, 61, 82, 10, 351, 73, 1, 1200, 2, 1, 20, 45],\n",
              " [1201, 33],\n",
              " [1201, 33, 1],\n",
              " [1201, 33, 1, 1202],\n",
              " [1201, 33, 1, 1202, 22],\n",
              " [1201, 33, 1, 1202, 22, 349],\n",
              " [1201, 33, 1, 1202, 22, 349, 23],\n",
              " [1201, 33, 1, 1202, 22, 349, 23, 1203],\n",
              " [1201, 33, 1, 1202, 22, 349, 23, 1203, 10],\n",
              " [1201, 33, 1, 1202, 22, 349, 23, 1203, 10, 53],\n",
              " [1201, 33, 1, 1202, 22, 349, 23, 1203, 10, 53, 706],\n",
              " [1201, 33, 1, 1202, 22, 349, 23, 1203, 10, 53, 706, 8],\n",
              " [1204, 4],\n",
              " [1204, 4, 1],\n",
              " [1204, 4, 1, 352],\n",
              " [1204, 4, 1, 352, 1205],\n",
              " [1204, 4, 1, 352, 1205, 198],\n",
              " [87, 1],\n",
              " [87, 1, 143],\n",
              " [87, 1, 143, 4],\n",
              " [87, 1, 143, 4, 61],\n",
              " [87, 1, 143, 4, 61, 82],\n",
              " [87, 1, 143, 4, 61, 82, 10],\n",
              " [87, 1, 143, 4, 61, 82, 10, 473],\n",
              " [87, 1, 143, 4, 61, 82, 10, 473, 161],\n",
              " [87, 1, 143, 4, 61, 82, 10, 473, 161, 12],\n",
              " [87, 1, 143, 4, 61, 82, 10, 473, 161, 12, 35],\n",
              " [87, 1, 143, 4, 61, 82, 10, 473, 161, 12, 35, 8],\n",
              " [87, 1, 143, 4, 61, 82, 10, 473, 161, 12, 35, 8, 707],\n",
              " [87, 1, 143, 4, 61, 82, 10, 473, 161, 12, 35, 8, 707, 4],\n",
              " [708, 4],\n",
              " [708, 4, 1],\n",
              " [708, 4, 1, 1206],\n",
              " [708, 4, 1, 1206, 287],\n",
              " [708, 4, 1, 1206, 287, 199],\n",
              " [708, 4, 1, 1206, 287, 199, 9],\n",
              " [708, 4, 1, 1206, 287, 199, 9, 35],\n",
              " [708, 4, 1, 1206, 287, 199, 9, 35, 10],\n",
              " [708, 4, 1, 1206, 287, 199, 9, 35, 10, 709],\n",
              " [708, 4, 1, 1206, 287, 199, 9, 35, 10, 709, 353],\n",
              " [4, 50],\n",
              " [4, 50, 3],\n",
              " [4, 50, 3, 199],\n",
              " [4, 50, 3, 199, 9],\n",
              " [4, 50, 3, 199, 9, 25],\n",
              " [4, 50, 3, 199, 9, 25, 10],\n",
              " [4, 50, 3, 199, 9, 25, 10, 1207],\n",
              " [4, 50, 3, 199, 9, 25, 10, 1207, 4],\n",
              " [4, 50, 3, 199, 9, 25, 10, 1207, 4, 1],\n",
              " [4, 50, 3, 199, 9, 25, 10, 1207, 4, 1, 288],\n",
              " [4, 50, 3, 199, 9, 25, 10, 1207, 4, 1, 288, 12],\n",
              " [4, 50, 3, 199, 9, 25, 10, 1207, 4, 1, 288, 12, 474],\n",
              " [4, 50, 3, 199, 9, 25, 10, 1207, 4, 1, 288, 12, 474, 15],\n",
              " [162, 70],\n",
              " [162, 70, 1],\n",
              " [162, 70, 1, 1208],\n",
              " [162, 70, 1, 1208, 112],\n",
              " [162, 70, 1, 1208, 112, 710],\n",
              " [87, 4],\n",
              " [87, 4, 289],\n",
              " [87, 4, 289, 290],\n",
              " [87, 4, 289, 290, 3],\n",
              " [87, 4, 289, 290, 3, 475],\n",
              " [87, 4, 289, 290, 3, 475, 711],\n",
              " [87, 4, 289, 290, 3, 475, 711, 103],\n",
              " [87, 4, 289, 290, 3, 475, 711, 103, 9],\n",
              " [87, 4, 289, 290, 3, 475, 711, 103, 9, 476],\n",
              " [163, 164],\n",
              " [163, 164, 5],\n",
              " [163, 164, 5, 4],\n",
              " [163, 164, 5, 4, 1],\n",
              " [163, 164, 5, 4, 1, 354],\n",
              " [163, 164, 5, 4, 1, 354, 4],\n",
              " [163, 164, 5, 4, 1, 354, 4, 712],\n",
              " [163, 164, 5, 4, 1, 354, 4, 712, 477],\n",
              " [163, 164, 5, 4, 1, 354, 4, 712, 477, 231],\n",
              " [163, 164, 5, 4, 1, 354, 4, 712, 477, 231, 9],\n",
              " [1, 113],\n",
              " [1, 113, 22],\n",
              " [1, 113, 22, 88],\n",
              " [1, 113, 22, 88, 23],\n",
              " [1, 113, 22, 88, 23, 65],\n",
              " [1, 113, 22, 88, 23, 65, 8],\n",
              " [1, 113, 22, 88, 23, 65, 8, 232],\n",
              " [1, 113, 22, 88, 23, 65, 8, 232, 4],\n",
              " [1, 113, 22, 88, 23, 65, 8, 232, 4, 291],\n",
              " [1, 113, 22, 88, 23, 65, 8, 232, 4, 291, 233],\n",
              " [1, 113, 22, 88, 23, 65, 8, 232, 4, 291, 233, 41],\n",
              " [234, 162],\n",
              " [234, 162, 9],\n",
              " [234, 162, 9, 1209],\n",
              " [234, 162, 9, 1209, 35],\n",
              " [234, 162, 9, 1209, 35, 200],\n",
              " [87, 13],\n",
              " [87, 13, 10],\n",
              " [87, 13, 10, 165],\n",
              " [87, 13, 10, 165, 4],\n",
              " [87, 13, 10, 165, 4, 355],\n",
              " [87, 13, 10, 165, 4, 355, 8],\n",
              " [87, 13, 10, 165, 4, 355, 8, 201],\n",
              " [87, 13, 10, 165, 4, 355, 8, 201, 202],\n",
              " [87, 13, 10, 165, 4, 355, 8, 201, 202, 166],\n",
              " [87, 13, 10, 165, 4, 355, 8, 201, 202, 166, 3],\n",
              " [87, 13, 10, 165, 4, 355, 8, 201, 202, 166, 3, 356],\n",
              " [87, 13, 10, 165, 4, 355, 8, 201, 202, 166, 3, 356, 4],\n",
              " [713, 714],\n",
              " [713, 714, 357],\n",
              " [713, 714, 357, 2],\n",
              " [713, 714, 357, 2, 89],\n",
              " [713, 714, 357, 2, 89, 25],\n",
              " [713, 714, 357, 2, 89, 25, 10],\n",
              " [713, 714, 357, 2, 89, 25, 10, 20],\n",
              " [713, 714, 357, 2, 89, 25, 10, 20, 45],\n",
              " [713, 714, 357, 2, 89, 25, 10, 20, 45, 90],\n",
              " [713, 714, 357, 2, 89, 25, 10, 20, 45, 90, 15],\n",
              " [713, 714, 357, 2, 89, 25, 10, 20, 45, 90, 15, 104],\n",
              " [713, 714, 357, 2, 89, 25, 10, 20, 45, 90, 15, 104, 4],\n",
              " [478, 292],\n",
              " [87, 5],\n",
              " [87, 5, 10],\n",
              " [87, 5, 10, 1210],\n",
              " [87, 5, 10, 1210, 34],\n",
              " [87, 5, 10, 1210, 34, 105],\n",
              " [87, 5, 10, 1210, 34, 105, 1211],\n",
              " [87, 5, 10, 1210, 34, 105, 1211, 70],\n",
              " [87, 5, 10, 1210, 34, 105, 1211, 70, 358],\n",
              " [87, 5, 10, 1210, 34, 105, 1211, 70, 358, 235],\n",
              " [87, 5, 10, 1210, 34, 105, 1211, 70, 358, 235, 359],\n",
              " [87, 5, 10, 1210, 34, 105, 1211, 70, 358, 235, 359, 715],\n",
              " [87, 5, 10, 1210, 34, 105, 1211, 70, 358, 235, 359, 715, 1212],\n",
              " [293, 3],\n",
              " [293, 3, 358],\n",
              " [293, 3, 358, 235],\n",
              " [293, 3, 358, 235, 359],\n",
              " [293, 3, 358, 235, 359, 716],\n",
              " [293, 3, 358, 235, 359, 716, 479],\n",
              " [293, 3, 358, 235, 359, 716, 479, 8],\n",
              " [293, 3, 358, 235, 359, 716, 479, 8, 717],\n",
              " [293, 3, 358, 235, 359, 716, 479, 8, 717, 4],\n",
              " [293, 3, 358, 235, 359, 716, 479, 8, 717, 4, 1213],\n",
              " [293, 3, 358, 235, 359, 716, 479, 8, 717, 4, 1213, 3],\n",
              " [293, 3, 358, 235, 359, 716, 479, 8, 717, 4, 1213, 3, 112],\n",
              " [1, 360],\n",
              " [1, 360, 8],\n",
              " [1, 360, 8, 718],\n",
              " [1, 360, 8, 718, 3],\n",
              " [1, 360, 8, 718, 3, 719],\n",
              " [1, 360, 8, 718, 3, 719, 70],\n",
              " [1, 360, 8, 718, 3, 719, 70, 13],\n",
              " [1, 360, 8, 718, 3, 719, 70, 13, 720],\n",
              " [1, 360, 8, 718, 3, 719, 70, 13, 720, 3],\n",
              " [1, 360, 8, 718, 3, 719, 70, 13, 720, 3, 13],\n",
              " [721, 361],\n",
              " [721, 361, 32],\n",
              " [721, 361, 32, 722],\n",
              " [721, 361, 32, 722, 1214],\n",
              " [721, 361, 32, 722, 1214, 26],\n",
              " [721, 361, 32, 722, 1214, 26, 1],\n",
              " [721, 361, 32, 722, 1214, 26, 1, 1215],\n",
              " [721, 361, 32, 722, 1214, 26, 1, 1215, 2],\n",
              " [721, 361, 32, 722, 1214, 26, 1, 1215, 2, 1],\n",
              " [1216, 722],\n",
              " [1216, 722, 1217],\n",
              " [1216, 722, 1217, 70],\n",
              " [1216, 722, 1217, 70, 54],\n",
              " [1216, 722, 1217, 70, 54, 24],\n",
              " [1216, 722, 1217, 70, 54, 24, 13],\n",
              " [1216, 722, 1217, 70, 54, 24, 13, 362],\n",
              " [1216, 722, 1217, 70, 54, 24, 13, 362, 3],\n",
              " [1216, 722, 1217, 70, 54, 24, 13, 362, 3, 54],\n",
              " [1216, 722, 1217, 70, 54, 24, 13, 362, 3, 54, 55],\n",
              " [236, 13],\n",
              " [236, 13, 362],\n",
              " [236, 13, 362, 5],\n",
              " [87, 13],\n",
              " [87, 13, 363],\n",
              " [87, 13, 363, 3],\n",
              " [87, 13, 363, 3, 121],\n",
              " [87, 13, 363, 3, 121, 106],\n",
              " [87, 13, 363, 3, 121, 106, 237],\n",
              " [87, 13, 363, 3, 121, 106, 237, 1],\n",
              " [87, 13, 363, 3, 121, 106, 237, 1, 1218],\n",
              " [87, 13, 363, 3, 121, 106, 237, 1, 1218, 723],\n",
              " [87, 13, 363, 3, 121, 106, 237, 1, 1218, 723, 2],\n",
              " [87, 13, 363, 3, 121, 106, 237, 1, 1218, 723, 2, 54],\n",
              " [87, 13, 363, 3, 121, 106, 237, 1, 1218, 723, 2, 54, 55],\n",
              " [5, 31],\n",
              " [5, 31, 64],\n",
              " [5, 31, 64, 724],\n",
              " [5, 31, 64, 724, 8],\n",
              " [5, 31, 64, 724, 8, 294],\n",
              " [5, 31, 64, 724, 8, 294, 1219],\n",
              " [5, 31, 64, 724, 8, 294, 1219, 1],\n",
              " [5, 31, 64, 724, 8, 294, 1219, 1, 22],\n",
              " [5, 31, 64, 724, 8, 294, 1219, 1, 22, 13],\n",
              " [725, 23],\n",
              " [725, 23, 726],\n",
              " [725, 23, 726, 727],\n",
              " [725, 23, 726, 727, 8],\n",
              " [725, 23, 726, 727, 8, 1220],\n",
              " [725, 23, 726, 727, 8, 1220, 3],\n",
              " [725, 23, 726, 727, 8, 1220, 3, 480],\n",
              " [725, 23, 726, 727, 8, 1220, 3, 480, 10],\n",
              " [725, 23, 726, 727, 8, 1220, 3, 480, 10, 1],\n",
              " [725, 23, 726, 727, 8, 1220, 3, 480, 10, 1, 20],\n",
              " [725, 23, 726, 727, 8, 1220, 3, 480, 10, 1, 20, 45],\n",
              " [725, 23, 726, 727, 8, 1220, 3, 480, 10, 1, 20, 45, 728],\n",
              " [729, 15],\n",
              " [729, 15, 1221],\n",
              " [729, 15, 1221, 13],\n",
              " [729, 15, 1221, 13, 203],\n",
              " [729, 15, 1221, 13, 203, 295],\n",
              " [729, 15, 1221, 13, 203, 295, 107],\n",
              " [729, 15, 1221, 13, 203, 295, 107, 5],\n",
              " [729, 15, 1221, 13, 203, 295, 107, 5, 2],\n",
              " [729, 15, 1221, 13, 203, 295, 107, 5, 2, 1],\n",
              " [122, 10],\n",
              " [122, 10, 65],\n",
              " [122, 10, 65, 12],\n",
              " [122, 10, 65, 12, 7],\n",
              " [122, 10, 65, 12, 7, 730],\n",
              " [122, 10, 65, 12, 7, 730, 1222],\n",
              " [122, 10, 65, 12, 7, 730, 1222, 2],\n",
              " [122, 10, 65, 12, 7, 730, 1222, 2, 54],\n",
              " [122, 10, 65, 12, 7, 730, 1222, 2, 54, 55],\n",
              " [122, 10, 65, 12, 7, 730, 1222, 2, 54, 55, 5],\n",
              " [122, 10, 65, 12, 7, 730, 1222, 2, 54, 55, 5, 8],\n",
              " [469, 4],\n",
              " [469, 4, 731],\n",
              " [87, 732],\n",
              " [87, 732, 364],\n",
              " [87, 732, 364, 733],\n",
              " [87, 732, 364, 733, 2],\n",
              " [87, 732, 364, 733, 2, 1],\n",
              " [87, 732, 364, 733, 2, 1, 88],\n",
              " [87, 732, 364, 733, 2, 1, 88, 481],\n",
              " [87, 732, 364, 733, 2, 1, 88, 481, 10],\n",
              " [87, 732, 364, 733, 2, 1, 88, 481, 10, 65],\n",
              " [87, 732, 364, 733, 2, 1, 88, 481, 10, 65, 12],\n",
              " [87, 732, 364, 733, 2, 1, 88, 481, 10, 65, 12, 123],\n",
              " [87, 732, 364, 733, 2, 1, 88, 481, 10, 65, 12, 123, 12],\n",
              " [482, 3],\n",
              " [482, 3, 734],\n",
              " [482, 3, 734, 4],\n",
              " [482, 3, 734, 4, 735],\n",
              " [482, 3, 734, 4, 735, 8],\n",
              " [482, 3, 734, 4, 735, 8, 736],\n",
              " [482, 3, 734, 4, 735, 8, 736, 66],\n",
              " [482, 3, 734, 4, 735, 8, 736, 66, 231],\n",
              " [482, 3, 734, 4, 735, 8, 736, 66, 231, 9],\n",
              " [482, 3, 734, 4, 735, 8, 736, 66, 231, 9, 25],\n",
              " [482, 3, 734, 4, 735, 8, 736, 66, 231, 9, 25, 737],\n",
              " [28, 7],\n",
              " [28, 7, 1223],\n",
              " [28, 7, 1223, 2],\n",
              " [28, 7, 1223, 2, 1224],\n",
              " [28, 7, 1223, 2, 1224, 74],\n",
              " [28, 7, 1223, 2, 1224, 74, 106],\n",
              " [28, 7, 1223, 2, 1224, 74, 106, 738],\n",
              " [28, 7, 1223, 2, 1224, 74, 106, 738, 26],\n",
              " [28, 7, 1223, 2, 1224, 74, 106, 738, 26, 165],\n",
              " [28, 7, 1223, 2, 1224, 74, 106, 738, 26, 165, 8],\n",
              " [739, 3],\n",
              " [739, 3, 740],\n",
              " [739, 3, 740, 4],\n",
              " [739, 3, 740, 4, 483],\n",
              " [87, 1],\n",
              " [87, 1, 741],\n",
              " [87, 1, 741, 484],\n",
              " [87, 1, 741, 484, 2],\n",
              " [87, 1, 741, 484, 2, 13],\n",
              " [87, 1, 741, 484, 2, 13, 10],\n",
              " [87, 1, 741, 484, 2, 13, 10, 103],\n",
              " [87, 1, 741, 484, 2, 13, 10, 103, 4],\n",
              " [87, 1, 741, 484, 2, 13, 10, 103, 4, 485],\n",
              " [87, 1, 741, 484, 2, 13, 10, 103, 4, 485, 8],\n",
              " [87, 1, 741, 484, 2, 13, 10, 103, 4, 485, 8, 1225],\n",
              " [87, 1, 741, 484, 2, 13, 10, 103, 4, 485, 8, 1225, 3],\n",
              " [1, 486],\n",
              " [1, 486, 1226],\n",
              " [1, 486, 1226, 742],\n",
              " [1, 486, 1226, 742, 8],\n",
              " [1, 486, 1226, 742, 8, 1227],\n",
              " [1, 486, 1226, 742, 8, 1227, 4],\n",
              " [1, 486, 1226, 742, 8, 1227, 4, 1228],\n",
              " [1, 486, 1226, 742, 8, 1227, 4, 1228, 1229],\n",
              " [1, 486, 1226, 742, 8, 1227, 4, 1228, 1229, 10],\n",
              " [165, 8],\n",
              " [165, 8, 1230],\n",
              " [165, 8, 1230, 4],\n",
              " [165, 8, 1230, 4, 743],\n",
              " [165, 8, 1230, 4, 743, 3],\n",
              " [165, 8, 1230, 4, 743, 3, 1],\n",
              " [165, 8, 1230, 4, 743, 3, 1, 484],\n",
              " [165, 8, 1230, 4, 743, 3, 1, 484, 2],\n",
              " [165, 8, 1230, 4, 743, 3, 1, 484, 2, 1],\n",
              " [165, 8, 1230, 4, 743, 3, 1, 484, 2, 1, 167],\n",
              " [165, 8, 1230, 4, 743, 3, 1, 484, 2, 1, 167, 13],\n",
              " [1231, 8],\n",
              " [1231, 8, 1232],\n",
              " [1231, 8, 1232, 4],\n",
              " [1231, 8, 1232, 4, 1233],\n",
              " [1231, 8, 1232, 4, 1233, 1234],\n",
              " [1231, 8, 1232, 4, 1233, 1234, 3],\n",
              " [1231, 8, 1232, 4, 1233, 1234, 3, 1235],\n",
              " [1231, 8, 1232, 4, 1233, 1234, 3, 1235, 744],\n",
              " [1231, 8, 1232, 4, 1233, 1234, 3, 1235, 744, 1],\n",
              " [1236, 15],\n",
              " [1236, 15, 13],\n",
              " [1236, 15, 13, 4],\n",
              " [1236, 15, 13, 4, 1237],\n",
              " [1236, 15, 13, 4, 1237, 1],\n",
              " [1236, 15, 13, 4, 1237, 1, 365],\n",
              " [1236, 15, 13, 4, 1237, 1, 365, 2],\n",
              " [1236, 15, 13, 4, 1237, 1, 365, 2, 13],\n",
              " [1236, 15, 13, 4, 1237, 1, 365, 2, 13, 1238],\n",
              " [10, 1239],\n",
              " [10, 1239, 4],\n",
              " [10, 1239, 4, 745],\n",
              " [10, 1239, 4, 745, 8],\n",
              " [10, 1239, 4, 745, 8, 1240],\n",
              " [10, 1239, 4, 745, 8, 1240, 3],\n",
              " [10, 1239, 4, 745, 8, 1240, 3, 1],\n",
              " [10, 1239, 4, 745, 8, 1240, 3, 1, 1241],\n",
              " [10, 1239, 4, 745, 8, 1240, 3, 1, 1241, 746],\n",
              " [10, 1239, 4, 745, 8, 1240, 3, 1, 1241, 746, 10],\n",
              " [1242, 4],\n",
              " [1242, 4, 1243],\n",
              " [1242, 4, 1243, 8],\n",
              " [1242, 4, 1243, 8, 1244],\n",
              " [87, 1],\n",
              " [87, 1, 296],\n",
              " [87, 1, 296, 361],\n",
              " [87, 1, 296, 361, 2],\n",
              " [87, 1, 296, 361, 2, 61],\n",
              " [87, 1, 296, 361, 2, 61, 238],\n",
              " [87, 1, 296, 361, 2, 61, 238, 26],\n",
              " [87, 1, 296, 361, 2, 61, 238, 26, 65],\n",
              " [87, 1, 296, 361, 2, 61, 238, 26, 65, 4],\n",
              " [87, 1, 296, 361, 2, 61, 238, 26, 65, 4, 1],\n",
              " [87, 1, 296, 361, 2, 61, 238, 26, 65, 4, 1, 747],\n",
              " [239, 2],\n",
              " [239, 2, 1],\n",
              " [239, 2, 1, 487],\n",
              " [239, 2, 1, 487, 198],\n",
              " [239, 2, 1, 487, 198, 12],\n",
              " [239, 2, 1, 487, 198, 12, 26],\n",
              " [239, 2, 1, 487, 198, 12, 26, 1],\n",
              " [239, 2, 1, 487, 198, 12, 26, 1, 297],\n",
              " [239, 2, 1, 487, 198, 12, 26, 1, 297, 2],\n",
              " [239, 2, 1, 487, 198, 12, 26, 1, 297, 2, 488],\n",
              " [489, 10],\n",
              " [489, 10, 490],\n",
              " [489, 10, 490, 12],\n",
              " [489, 10, 490, 12, 7],\n",
              " [489, 10, 490, 12, 7, 240],\n",
              " [489, 10, 490, 12, 7, 240, 1245],\n",
              " [489, 10, 490, 12, 7, 240, 1245, 4],\n",
              " [489, 10, 490, 12, 7, 240, 1245, 4, 43],\n",
              " [489, 10, 490, 12, 7, 240, 1245, 4, 43, 11],\n",
              " [5, 4],\n",
              " [5, 4, 1],\n",
              " [5, 4, 1, 487],\n",
              " [5, 4, 1, 487, 198],\n",
              " [5, 4, 1, 487, 198, 3],\n",
              " [5, 4, 1, 487, 198, 3, 7],\n",
              " [5, 4, 1, 487, 198, 3, 7, 748],\n",
              " [5, 4, 1, 487, 198, 3, 7, 748, 54],\n",
              " [5, 4, 1, 487, 198, 3, 7, 748, 54, 2],\n",
              " [5, 4, 1, 487, 198, 3, 7, 748, 54, 2, 491],\n",
              " [5, 4, 1, 487, 198, 3, 7, 748, 54, 2, 491, 30],\n",
              " [5, 4, 1, 487, 198, 3, 7, 748, 54, 2, 491, 30, 10],\n",
              " [65, 4],\n",
              " [65, 4, 492],\n",
              " [65, 4, 492, 8],\n",
              " [65, 4, 492, 8, 298],\n",
              " [65, 4, 492, 8, 298, 3],\n",
              " [65, 4, 492, 8, 298, 3, 299],\n",
              " [65, 4, 492, 8, 298, 3, 299, 1],\n",
              " [65, 4, 492, 8, 298, 3, 299, 1, 300],\n",
              " [65, 4, 492, 8, 298, 3, 299, 1, 300, 2],\n",
              " [65, 4, 492, 8, 298, 3, 299, 1, 300, 2, 7],\n",
              " [65, 4, 492, 8, 298, 3, 299, 1, 300, 2, 7, 241],\n",
              " [65, 4, 492, 8, 298, 3, 299, 1, 300, 2, 7, 241, 52],\n",
              " [61, 749],\n",
              " [61, 749, 10],\n",
              " [61, 749, 10, 71],\n",
              " [61, 749, 10, 71, 8],\n",
              " [61, 749, 10, 71, 8, 1246],\n",
              " [61, 749, 10, 71, 8, 1246, 4],\n",
              " [61, 749, 10, 71, 8, 1246, 4, 1],\n",
              " [61, 749, 10, 71, 8, 1246, 4, 1, 123],\n",
              " [61, 749, 10, 71, 8, 1246, 4, 1, 123, 1247],\n",
              " [87, 750],\n",
              " [87, 750, 4],\n",
              " [87, 750, 4, 13],\n",
              " [87, 750, 4, 13, 1248],\n",
              " [87, 750, 4, 13, 1248, 31],\n",
              " [87, 750, 4, 13, 1248, 31, 751],\n",
              " [87, 750, 4, 13, 1248, 31, 751, 1],\n",
              " [87, 750, 4, 13, 1248, 31, 751, 1, 1249],\n",
              " [87, 750, 4, 13, 1248, 31, 751, 1, 1249, 2],\n",
              " [493, 203],\n",
              " [493, 203, 204],\n",
              " [493, 203, 204, 4],\n",
              " [493, 203, 204, 4, 1],\n",
              " [493, 203, 204, 4, 1, 360],\n",
              " [493, 203, 204, 4, 1, 360, 3],\n",
              " [493, 203, 204, 4, 1, 360, 3, 752],\n",
              " [493, 203, 204, 4, 1, 360, 3, 752, 753],\n",
              " [493, 203, 204, 4, 1, 360, 3, 752, 753, 167],\n",
              " [13, 4],\n",
              " [13, 4, 483],\n",
              " [13, 4, 483, 3],\n",
              " [13, 4, 483, 3, 4],\n",
              " [13, 4, 483, 3, 4, 1],\n",
              " [13, 4, 483, 3, 4, 1, 1250],\n",
              " [13, 4, 483, 3, 4, 1, 1250, 1251],\n",
              " [13, 4, 483, 3, 4, 1, 1250, 1251, 13],\n",
              " [13, 4, 483, 3, 4, 1, 1250, 1251, 13, 754],\n",
              " [13, 4, 483, 3, 4, 1, 1250, 1251, 13, 754, 8],\n",
              " [13, 4, 483, 3, 4, 1, 1250, 1251, 13, 754, 8, 755],\n",
              " [756, 757],\n",
              " [87, 1],\n",
              " [87, 1, 20],\n",
              " [87, 1, 20, 45],\n",
              " [87, 1, 20, 45, 758],\n",
              " [87, 1, 20, 45, 758, 759],\n",
              " [87, 1, 20, 45, 758, 759, 10],\n",
              " [87, 1, 20, 45, 758, 759, 10, 301],\n",
              " [87, 1, 20, 45, 758, 759, 10, 301, 4],\n",
              " [87, 1, 20, 45, 758, 759, 10, 301, 4, 485],\n",
              " [87, 1, 20, 45, 758, 759, 10, 301, 4, 485, 242],\n",
              " [87, 1, 20, 45, 758, 759, 10, 301, 4, 485, 242, 8],\n",
              " [494, 4],\n",
              " [494, 4, 760],\n",
              " [494, 4, 760, 3],\n",
              " [494, 4, 760, 3, 1252],\n",
              " [494, 4, 760, 3, 1252, 4],\n",
              " [494, 4, 760, 3, 1252, 4, 495],\n",
              " [494, 4, 760, 3, 1252, 4, 495, 1],\n",
              " [494, 4, 760, 3, 1252, 4, 495, 1, 761],\n",
              " [496, 497],\n",
              " [496, 497, 498],\n",
              " [496, 497, 498, 4],\n",
              " [496, 497, 498, 4, 499],\n",
              " [496, 497, 498, 4, 499, 3],\n",
              " [496, 497, 498, 4, 499, 3, 500],\n",
              " [496, 497, 498, 4, 499, 3, 500, 4],\n",
              " [496, 497, 498, 4, 499, 3, 500, 4, 495],\n",
              " [87, 9],\n",
              " [87, 9, 1253],\n",
              " [87, 9, 1253, 35],\n",
              " [87, 9, 1253, 35, 83],\n",
              " [87, 9, 1253, 35, 83, 4],\n",
              " [87, 9, 1253, 35, 83, 4, 124],\n",
              " [87, 9, 1253, 35, 83, 4, 124, 54],\n",
              " [87, 9, 1253, 35, 83, 4, 124, 54, 24],\n",
              " [87, 9, 1253, 35, 83, 4, 124, 54, 24, 3],\n",
              " [87, 9, 1253, 35, 83, 4, 124, 54, 24, 3, 54],\n",
              " [87, 9, 1253, 35, 83, 4, 124, 54, 24, 3, 54, 55],\n",
              " [87, 9, 1253, 35, 83, 4, 124, 54, 24, 3, 54, 55, 5],\n",
              " [87, 9, 1253, 35, 83, 4, 124, 54, 24, 3, 54, 55, 5, 10],\n",
              " [366, 1254],\n",
              " [366, 1254, 10],\n",
              " [366, 1254, 10, 501],\n",
              " [366, 1254, 10, 501, 8],\n",
              " [366, 1254, 10, 501, 8, 1],\n",
              " [366, 1254, 10, 501, 8, 1, 5],\n",
              " [366, 1254, 10, 501, 8, 1, 5, 83],\n",
              " [366, 1254, 10, 501, 8, 1, 5, 83, 3],\n",
              " [366, 1254, 10, 501, 8, 1, 5, 83, 3, 40],\n",
              " [366, 1254, 10, 501, 8, 1, 5, 83, 3, 40, 502],\n",
              " [762, 3],\n",
              " [762, 3, 1],\n",
              " [762, 3, 1, 470],\n",
              " [762, 3, 1, 470, 503],\n",
              " [762, 3, 1, 470, 503, 5],\n",
              " [762, 3, 1, 470, 503, 5, 168],\n",
              " [762, 3, 1, 470, 503, 5, 168, 1255],\n",
              " [87, 763],\n",
              " [87, 763, 4],\n",
              " [87, 763, 4, 1],\n",
              " [87, 763, 4, 1, 104],\n",
              " [87, 763, 4, 1, 104, 2],\n",
              " [87, 763, 4, 1, 104, 2, 40],\n",
              " [87, 763, 4, 1, 104, 2, 40, 205],\n",
              " [87, 763, 4, 1, 104, 2, 40, 205, 1256],\n",
              " [15, 238],\n",
              " [15, 238, 20],\n",
              " [15, 238, 20, 45],\n",
              " [15, 238, 20, 45, 65],\n",
              " [15, 238, 20, 45, 65, 8],\n",
              " [15, 238, 20, 45, 65, 8, 504],\n",
              " [15, 238, 20, 45, 65, 8, 504, 1257],\n",
              " [15, 238, 20, 45, 65, 8, 504, 1257, 1],\n",
              " [15, 238, 20, 45, 65, 8, 504, 1257, 1, 505],\n",
              " [2, 50],\n",
              " [2, 50, 764],\n",
              " [2, 50, 764, 6],\n",
              " [2, 50, 764, 6, 1258],\n",
              " [2, 50, 764, 6, 1258, 1],\n",
              " [2, 50, 764, 6, 1258, 1, 302],\n",
              " [2, 50, 764, 6, 1258, 1, 302, 2],\n",
              " [2, 50, 764, 6, 1258, 1, 302, 2, 243],\n",
              " [506, 8],\n",
              " [506, 8, 1259],\n",
              " [506, 8, 1259, 3],\n",
              " [506, 8, 1259, 3, 1260],\n",
              " [506, 8, 1259, 3, 1260, 1],\n",
              " [506, 8, 1259, 3, 1260, 1, 367],\n",
              " [506, 8, 1259, 3, 1260, 1, 367, 2],\n",
              " [506, 8, 1259, 3, 1260, 1, 367, 2, 169],\n",
              " [506, 8, 1259, 3, 1260, 1, 367, 2, 169, 765],\n",
              " [13, 4],\n",
              " [13, 4, 1],\n",
              " [13, 4, 1, 104],\n",
              " [13, 4, 1, 104, 2],\n",
              " [13, 4, 1, 104, 2, 61],\n",
              " [13, 4, 1, 104, 2, 61, 507],\n",
              " [13, 4, 1, 104, 2, 61, 507, 4],\n",
              " [13, 4, 1, 104, 2, 61, 507, 4, 1],\n",
              " [13, 4, 1, 104, 2, 61, 507, 4, 1, 508],\n",
              " [13, 4, 1, 104, 2, 61, 507, 4, 1, 508, 3],\n",
              " [1261, 4],\n",
              " [1261, 4, 1],\n",
              " [1261, 4, 1, 36],\n",
              " [1261, 4, 1, 36, 2],\n",
              " [1261, 4, 1, 36, 2, 1262],\n",
              " [1261, 4, 1, 36, 2, 1262, 1263],\n",
              " [1261, 4, 1, 36, 2, 1262, 1263, 11],\n",
              " [1261, 4, 1, 36, 2, 1262, 1263, 11, 5],\n",
              " [1261, 4, 1, 36, 2, 1262, 1263, 11, 5, 1264],\n",
              " [8, 1265],\n",
              " [8, 1265, 3],\n",
              " [8, 1265, 3, 1266],\n",
              " [1267, 206],\n",
              " [1267, 206, 1268],\n",
              " [197, 11],\n",
              " [197, 11, 1],\n",
              " [197, 11, 1, 1269],\n",
              " [197, 11, 1, 1269, 766],\n",
              " [197, 11, 1, 1269, 766, 2],\n",
              " [197, 11, 1, 1269, 766, 2, 5],\n",
              " [197, 11, 1, 1269, 766, 2, 5, 349],\n",
              " [197, 11, 1, 1269, 766, 2, 5, 349, 26],\n",
              " [197, 11, 1, 1269, 766, 2, 5, 349, 26, 351],\n",
              " [197, 11, 1, 1269, 766, 2, 5, 349, 26, 351, 4],\n",
              " [197, 11, 1, 1269, 766, 2, 5, 349, 26, 351, 4, 1270],\n",
              " [197, 11, 1, 1269, 766, 2, 5, 349, 26, 351, 4, 1270, 244],\n",
              " [197, 11, 1, 1269, 766, 2, 5, 349, 26, 351, 4, 1270, 244, 24],\n",
              " [197, 11, 1, 1269, 766, 2, 5, 349, 26, 351, 4, 1270, 244, 24, 24],\n",
              " [197, 11, 1, 1269, 766, 2, 5, 349, 26, 351, 4, 1270, 244, 24, 24, 7],\n",
              " [197, 11, 1, 1269, 766, 2, 5, 349, 26, 351, 4, 1270, 244, 24, 24, 7, 703],\n",
              " [197,\n",
              "  11,\n",
              "  1,\n",
              "  1269,\n",
              "  766,\n",
              "  2,\n",
              "  5,\n",
              "  349,\n",
              "  26,\n",
              "  351,\n",
              "  4,\n",
              "  1270,\n",
              "  244,\n",
              "  24,\n",
              "  24,\n",
              "  7,\n",
              "  703,\n",
              "  1271],\n",
              " [197,\n",
              "  11,\n",
              "  1,\n",
              "  1269,\n",
              "  766,\n",
              "  2,\n",
              "  5,\n",
              "  349,\n",
              "  26,\n",
              "  351,\n",
              "  4,\n",
              "  1270,\n",
              "  244,\n",
              "  24,\n",
              "  24,\n",
              "  7,\n",
              "  703,\n",
              "  1271,\n",
              "  10],\n",
              " [197,\n",
              "  11,\n",
              "  1,\n",
              "  1269,\n",
              "  766,\n",
              "  2,\n",
              "  5,\n",
              "  349,\n",
              "  26,\n",
              "  351,\n",
              "  4,\n",
              "  1270,\n",
              "  244,\n",
              "  24,\n",
              "  24,\n",
              "  7,\n",
              "  703,\n",
              "  1271,\n",
              "  10,\n",
              "  65],\n",
              " [4, 41],\n",
              " [4, 41, 1272],\n",
              " [4, 41, 1272, 1273],\n",
              " [4, 41, 1272, 1273, 1274],\n",
              " [4, 41, 1272, 1273, 1274, 18],\n",
              " [4, 41, 1272, 1273, 1274, 18, 52],\n",
              " [4, 41, 1272, 1273, 1274, 18, 52, 1275],\n",
              " [4, 41, 1272, 1273, 1274, 18, 52, 1275, 1276],\n",
              " [4, 41, 1272, 1273, 1274, 18, 52, 1275, 1276, 165],\n",
              " [4, 41, 1272, 1273, 1274, 18, 52, 1275, 1276, 165, 8],\n",
              " [767, 1277],\n",
              " [767, 1277, 78],\n",
              " [767, 1277, 78, 24],\n",
              " [767, 1277, 78, 24, 24],\n",
              " [767, 1277, 78, 24, 24, 3],\n",
              " [767, 1277, 78, 24, 24, 3, 7],\n",
              " [767, 1277, 78, 24, 24, 3, 7, 1278],\n",
              " [767, 1277, 78, 24, 24, 3, 7, 1278, 1279],\n",
              " [767, 1277, 78, 24, 24, 3, 7, 1278, 1279, 1280],\n",
              " [2, 509],\n",
              " [2, 509, 170],\n",
              " [2, 509, 170, 510],\n",
              " [2, 509, 170, 510, 28],\n",
              " [2, 509, 170, 510, 28, 511],\n",
              " [2, 509, 170, 510, 28, 511, 54],\n",
              " [2, 509, 170, 510, 28, 511, 54, 24],\n",
              " [2, 509, 170, 510, 28, 511, 54, 24, 5],\n",
              " [2, 509, 170, 510, 28, 511, 54, 24, 5, 10],\n",
              " [2, 509, 170, 510, 28, 511, 54, 24, 5, 10, 144],\n",
              " [2, 509, 170, 510, 28, 511, 54, 24, 5, 10, 144, 8],\n",
              " [2, 509, 170, 510, 28, 511, 54, 24, 5, 10, 144, 8, 768],\n",
              " [2, 1281],\n",
              " [2, 1281, 4],\n",
              " [2, 1281, 4, 1],\n",
              " [2, 1281, 4, 1, 1282],\n",
              " [2, 1281, 4, 1, 1282, 198],\n",
              " [2, 1281, 4, 1, 1282, 198, 1283],\n",
              " [2, 1281, 4, 1, 1282, 198, 1283, 78],\n",
              " [2, 1281, 4, 1, 1282, 198, 1283, 78, 24],\n",
              " [2, 1281, 4, 1, 1282, 198, 1283, 78, 24, 55],\n",
              " [2, 1281, 4, 1, 1282, 198, 1283, 78, 24, 55, 7],\n",
              " [2, 1281, 4, 1, 1282, 198, 1283, 78, 24, 55, 7, 768],\n",
              " [2, 1281, 4, 1, 1282, 198, 1283, 78, 24, 55, 7, 768, 10],\n",
              " [1, 20],\n",
              " [1, 20, 45],\n",
              " [1, 20, 45, 6],\n",
              " [1, 20, 45, 6, 114],\n",
              " [1, 20, 45, 6, 114, 1],\n",
              " [1, 20, 45, 6, 114, 1, 207],\n",
              " [1, 20, 45, 6, 114, 1, 207, 22],\n",
              " [1, 20, 45, 6, 114, 1, 207, 22, 5],\n",
              " [1, 20, 45, 6, 114, 1, 207, 22, 5, 23],\n",
              " [1, 20, 45, 6, 114, 1, 207, 22, 5, 23, 18],\n",
              " [1, 20, 45, 6, 114, 1, 207, 22, 5, 23, 18, 1],\n",
              " [1, 20, 45, 6, 114, 1, 207, 22, 5, 23, 18, 1, 704],\n",
              " [1, 20, 45, 6, 114, 1, 207, 22, 5, 23, 18, 1, 704, 472],\n",
              " [1, 20, 45, 6, 114, 1, 207, 22, 5, 23, 18, 1, 704, 472, 15],\n",
              " [1, 20, 45, 6, 114, 1, 207, 22, 5, 23, 18, 1, 704, 472, 15, 7],\n",
              " [705, 22],\n",
              " [705, 22, 59],\n",
              " [705, 22, 59, 1],\n",
              " [705, 22, 59, 1, 171],\n",
              " [705, 22, 59, 1, 171, 1284],\n",
              " [705, 22, 59, 1, 171, 1284, 368],\n",
              " [705, 22, 59, 1, 171, 1284, 368, 46],\n",
              " [705, 22, 59, 1, 171, 1284, 368, 46, 769],\n",
              " [705, 22, 59, 1, 171, 1284, 368, 46, 769, 4],\n",
              " [705, 22, 59, 1, 171, 1284, 368, 46, 769, 4, 1],\n",
              " [705, 22, 59, 1, 171, 1284, 368, 46, 769, 4, 1, 770],\n",
              " [705, 22, 59, 1, 171, 1284, 368, 46, 769, 4, 1, 770, 33],\n",
              " [705, 22, 59, 1, 171, 1284, 368, 46, 769, 4, 1, 770, 33, 1285],\n",
              " [1, 1286],\n",
              " [1, 1286, 56],\n",
              " [1, 1286, 56, 44],\n",
              " [1, 1286, 56, 44, 770],\n",
              " [1, 1286, 56, 44, 770, 12],\n",
              " [1, 1286, 56, 44, 770, 12, 7],\n",
              " [1, 1286, 56, 44, 770, 12, 7, 1287],\n",
              " [1, 1286, 56, 44, 770, 12, 7, 1287, 771],\n",
              " [1, 1286, 56, 44, 770, 12, 7, 1287, 771, 6],\n",
              " [1, 1286, 56, 44, 770, 12, 7, 1287, 771, 6, 772],\n",
              " [1, 1286, 56, 44, 770, 12, 7, 1287, 771, 6, 772, 25],\n",
              " [1, 1286, 56, 44, 770, 12, 7, 1287, 771, 6, 772, 25, 23],\n",
              " [1, 1286, 56, 44, 770, 12, 7, 1287, 771, 6, 772, 25, 23, 172],\n",
              " [1, 1286, 56, 44, 770, 12, 7, 1287, 771, 6, 772, 25, 23, 172, 1288],\n",
              " [303, 2],\n",
              " [303, 2, 1],\n",
              " [303, 2, 1, 30],\n",
              " [303, 2, 1, 30, 734],\n",
              " [303, 2, 1, 30, 734, 1],\n",
              " [303, 2, 1, 30, 734, 1, 773],\n",
              " [303, 2, 1, 30, 734, 1, 773, 171],\n",
              " [303, 2, 1, 30, 734, 1, 773, 171, 369],\n",
              " [303, 2, 1, 30, 734, 1, 773, 171, 369, 2],\n",
              " [303, 2, 1, 30, 734, 1, 773, 171, 369, 2, 82],\n",
              " [5, 16],\n",
              " [5, 16, 7],\n",
              " [5, 16, 7, 1289],\n",
              " [5, 16, 7, 1289, 1290],\n",
              " [5, 16, 7, 1289, 1290, 46],\n",
              " [5, 16, 7, 1289, 1290, 46, 304],\n",
              " [5, 16, 7, 1289, 1290, 46, 304, 1291],\n",
              " [5, 16, 7, 1289, 1290, 46, 304, 1291, 370],\n",
              " [774, 173],\n",
              " [774, 173, 7],\n",
              " [774, 173, 7, 1292],\n",
              " [774, 173, 7, 1292, 512],\n",
              " [774, 173, 7, 1292, 512, 2],\n",
              " [774, 173, 7, 1292, 512, 2, 1],\n",
              " [774, 173, 7, 1292, 512, 2, 1, 1293],\n",
              " [774, 173, 7, 1292, 512, 2, 1, 1293, 3],\n",
              " [774, 173, 7, 1292, 512, 2, 1, 1293, 3, 1294],\n",
              " [774, 173, 7, 1292, 512, 2, 1, 1293, 3, 1294, 70],\n",
              " [82, 1],\n",
              " [82, 1, 43],\n",
              " [82, 1, 43, 1295],\n",
              " [82, 1, 43, 1295, 775],\n",
              " [82, 1, 43, 1295, 775, 513],\n",
              " [82, 1, 43, 1295, 775, 513, 776],\n",
              " [82, 1, 43, 1295, 775, 513, 776, 3],\n",
              " [82, 1, 43, 1295, 775, 513, 776, 3, 1],\n",
              " [82, 1, 43, 1295, 775, 513, 776, 3, 1, 1296],\n",
              " [82, 1, 43, 1295, 775, 513, 776, 3, 1, 1296, 16],\n",
              " [773, 514],\n",
              " [773, 514, 1],\n",
              " [773, 514, 1, 1297],\n",
              " [773, 514, 1, 1297, 2],\n",
              " [773, 514, 1, 1297, 2, 1298],\n",
              " [773, 514, 1, 1297, 2, 1298, 91],\n",
              " [773, 514, 1, 1297, 2, 1298, 91, 16],\n",
              " [773, 514, 1, 1297, 2, 1298, 91, 16, 371],\n",
              " [1299, 3],\n",
              " [1299, 3, 1300],\n",
              " [1299, 3, 1300, 777],\n",
              " [1299, 3, 1300, 777, 1301],\n",
              " [1299, 3, 1300, 777, 1301, 515],\n",
              " [1302, 3],\n",
              " [1302, 3, 1303],\n",
              " [1302, 3, 1303, 6],\n",
              " [1302, 3, 1303, 6, 1],\n",
              " [1302, 3, 1303, 6, 1, 245],\n",
              " [1302, 3, 1303, 6, 1, 245, 1304],\n",
              " [1302, 3, 1303, 6, 1, 245, 1304, 2],\n",
              " [82, 15],\n",
              " [82, 15, 1305],\n",
              " [82, 15, 1305, 47],\n",
              " [82, 15, 1305, 47, 82],\n",
              " [82, 15, 1305, 47, 82, 16],\n",
              " [82, 15, 1305, 47, 82, 16, 1306],\n",
              " [82, 15, 1305, 47, 82, 16, 1306, 75],\n",
              " [82, 15, 1305, 47, 82, 16, 1306, 75, 1307],\n",
              " [82, 15, 1305, 47, 82, 16, 1306, 75, 1307, 775],\n",
              " [82, 15, 1305, 47, 82, 16, 1306, 75, 1307, 775, 246],\n",
              " [247, 18],\n",
              " [247, 18, 515],\n",
              " [247, 18, 515, 21],\n",
              " [247, 18, 515, 21, 513],\n",
              " [247, 18, 515, 21, 513, 776],\n",
              " [247, 18, 515, 21, 513, 776, 516],\n",
              " [247, 18, 515, 21, 513, 776, 516, 15],\n",
              " [247, 18, 515, 21, 513, 776, 516, 15, 7],\n",
              " [247, 18, 515, 21, 513, 776, 516, 15, 7, 92],\n",
              " [247, 18, 515, 21, 513, 776, 516, 15, 7, 92, 67],\n",
              " [1308, 18],\n",
              " [1308, 18, 515],\n",
              " [1308, 18, 515, 125],\n",
              " [1308, 18, 515, 125, 1309],\n",
              " [1308, 18, 515, 125, 1309, 372],\n",
              " [1308, 18, 515, 125, 1309, 372, 1310],\n",
              " [1308, 18, 515, 125, 1309, 372, 1310, 3],\n",
              " [125, 778],\n",
              " [125, 778, 1311],\n",
              " [125, 778, 1311, 1],\n",
              " [125, 778, 1311, 1, 1312],\n",
              " [125, 778, 1311, 1, 1312, 1313],\n",
              " [125, 778, 1311, 1, 1312, 1313, 1314],\n",
              " [125, 778, 1311, 1, 1312, 1313, 1314, 174],\n",
              " [125, 778, 1311, 1, 1312, 1313, 1314, 174, 1],\n",
              " [125, 778, 1311, 1, 1312, 1313, 1314, 174, 1, 43],\n",
              " [17, 517],\n",
              " [17, 517, 8],\n",
              " [17, 517, 8, 1315],\n",
              " [17, 517, 8, 1315, 1316],\n",
              " [17, 517, 8, 1315, 1316, 3],\n",
              " [17, 517, 8, 1315, 1316, 3, 7],\n",
              " [17, 517, 8, 1315, 1316, 3, 7, 1317],\n",
              " [17, 517, 8, 1315, 1316, 3, 7, 1317, 777],\n",
              " [17, 517, 8, 1315, 1316, 3, 7, 1317, 777, 3],\n",
              " [248, 7],\n",
              " [248, 7, 371],\n",
              " [248, 7, 371, 208],\n",
              " [248, 7, 371, 208, 67],\n",
              " [248, 7, 371, 208, 67, 1318],\n",
              " [4, 708],\n",
              " [4, 708, 1],\n",
              " [4, 708, 1, 1319],\n",
              " [4, 708, 1, 1319, 1320],\n",
              " [4, 708, 1, 1319, 1320, 1321],\n",
              " [4, 708, 1, 1319, 1320, 1321, 707],\n",
              " [4, 708, 1, 1319, 1320, 1321, 707, 1322],\n",
              " [4, 708, 1, 1319, 1320, 1321, 707, 1322, 14],\n",
              " [4, 708, 1, 1319, 1320, 1321, 707, 1322, 14, 289],\n",
              " [4, 708, 1, 1319, 1320, 1321, 707, 1322, 14, 289, 501],\n",
              " [9, 1],\n",
              " [9, 1, 143],\n",
              " [9, 1, 143, 4],\n",
              " [9, 1, 143, 4, 61],\n",
              " [9, 1, 143, 4, 61, 82],\n",
              " [9, 1, 143, 4, 61, 82, 10],\n",
              " [9, 1, 143, 4, 61, 82, 10, 35],\n",
              " [9, 1, 143, 4, 61, 82, 10, 35, 76],\n",
              " [9, 1, 143, 4, 61, 82, 10, 35, 76, 4],\n",
              " [9, 1, 143, 4, 61, 82, 10, 35, 76, 4, 1],\n",
              " [9, 1, 143, 4, 61, 82, 10, 35, 76, 4, 1, 518],\n",
              " [9, 1, 143, 4, 61, 82, 10, 35, 76, 4, 1, 518, 2],\n",
              " [1, 198],\n",
              " [1, 198, 1323],\n",
              " [1, 198, 1323, 1],\n",
              " [1, 198, 1323, 1, 82],\n",
              " [1, 198, 1323, 1, 82, 6],\n",
              " [1, 198, 1323, 1, 82, 6, 1324],\n",
              " [1, 198, 1323, 1, 82, 6, 1324, 1],\n",
              " [1, 198, 1323, 1, 82, 6, 1324, 1, 209],\n",
              " [1, 198, 1323, 1, 82, 6, 1324, 1, 209, 10],\n",
              " [1, 198, 1323, 1, 82, 6, 1324, 1, 209, 10, 1325],\n",
              " [1, 198, 1323, 1, 82, 6, 1324, 1, 209, 10, 1325, 8],\n",
              " [1, 198, 1323, 1, 82, 6, 1324, 1, 209, 10, 1325, 8, 1326],\n",
              " [1, 198, 1323, 1, 82, 6, 1324, 1, 209, 10, 1325, 8, 1326, 779],\n",
              " [1, 198, 1323, 1, 82, 6, 1324, 1, 209, 10, 1325, 8, 1326, 779, 15],\n",
              " [1, 198, 1323, 1, 82, 6, 1324, 1, 209, 10, 1325, 8, 1326, 779, 15, 780],\n",
              " [1, 198, 1323, 1, 82, 6, 1324, 1, 209, 10, 1325, 8, 1326, 779, 15, 780, 519],\n",
              " [1,\n",
              "  198,\n",
              "  1323,\n",
              "  1,\n",
              "  82,\n",
              "  6,\n",
              "  1324,\n",
              "  1,\n",
              "  209,\n",
              "  10,\n",
              "  1325,\n",
              "  8,\n",
              "  1326,\n",
              "  779,\n",
              "  15,\n",
              "  780,\n",
              "  519,\n",
              "  48],\n",
              " [1,\n",
              "  198,\n",
              "  1323,\n",
              "  1,\n",
              "  82,\n",
              "  6,\n",
              "  1324,\n",
              "  1,\n",
              "  209,\n",
              "  10,\n",
              "  1325,\n",
              "  8,\n",
              "  1326,\n",
              "  779,\n",
              "  15,\n",
              "  780,\n",
              "  519,\n",
              "  48,\n",
              "  12],\n",
              " [1,\n",
              "  198,\n",
              "  1323,\n",
              "  1,\n",
              "  82,\n",
              "  6,\n",
              "  1324,\n",
              "  1,\n",
              "  209,\n",
              "  10,\n",
              "  1325,\n",
              "  8,\n",
              "  1326,\n",
              "  779,\n",
              "  15,\n",
              "  780,\n",
              "  519,\n",
              "  48,\n",
              "  12,\n",
              "  35],\n",
              " [1,\n",
              "  198,\n",
              "  1323,\n",
              "  1,\n",
              "  82,\n",
              "  6,\n",
              "  1324,\n",
              "  1,\n",
              "  209,\n",
              "  10,\n",
              "  1325,\n",
              "  8,\n",
              "  1326,\n",
              "  779,\n",
              "  15,\n",
              "  780,\n",
              "  519,\n",
              "  48,\n",
              "  12,\n",
              "  35,\n",
              "  12],\n",
              " [301, 8],\n",
              " [301, 8, 1327],\n",
              " [301, 8, 1327, 4],\n",
              " [301, 8, 1327, 4, 1328],\n",
              " [301, 8, 1327, 4, 1328, 1329],\n",
              " [301, 8, 1327, 4, 1328, 1329, 4],\n",
              " [301, 8, 1327, 4, 1328, 1329, 4, 1330],\n",
              " [301, 8, 1327, 4, 1328, 1329, 4, 1330, 3],\n",
              " [301, 8, 1327, 4, 1328, 1329, 4, 1330, 3, 14],\n",
              " [301, 8, 1327, 4, 1328, 1329, 4, 1330, 3, 14, 1],\n",
              " [301, 8, 1327, 4, 1328, 1329, 4, 1330, 3, 14, 1, 202],\n",
              " [305, 14],\n",
              " [305, 14, 1331],\n",
              " [305, 14, 1331, 4],\n",
              " [305, 14, 1331, 4, 781],\n",
              " [305, 14, 1331, 4, 781, 782],\n",
              " [305, 14, 1331, 4, 781, 782, 2],\n",
              " [305, 14, 1331, 4, 781, 782, 2, 50],\n",
              " [305, 14, 1331, 4, 781, 782, 2, 50, 35],\n",
              " [305, 14, 1331, 4, 781, 782, 2, 50, 35, 145],\n",
              " [53, 28],\n",
              " [53, 28, 783],\n",
              " [53, 28, 783, 8],\n",
              " [53, 28, 783, 8, 1332],\n",
              " [53, 28, 783, 8, 1332, 1333],\n",
              " [53, 28, 783, 8, 1332, 1333, 33],\n",
              " [53, 28, 783, 8, 1332, 1333, 33, 784],\n",
              " [53, 28, 783, 8, 1332, 1333, 33, 784, 373],\n",
              " [53, 28, 783, 8, 1332, 1333, 33, 784, 373, 306],\n",
              " [53, 28, 783, 8, 1332, 1333, 33, 784, 373, 306, 50],\n",
              " [53, 28, 783, 8, 1332, 1333, 33, 784, 373, 306, 50, 9],\n",
              " [25, 10],\n",
              " [25, 10, 1334],\n",
              " [25, 10, 1334, 90],\n",
              " [25, 10, 1334, 90, 4],\n",
              " [25, 10, 1334, 90, 4, 247],\n",
              " [25, 10, 1334, 90, 4, 247, 105],\n",
              " [25, 10, 1334, 90, 4, 247, 105, 36],\n",
              " [25, 10, 1334, 90, 4, 247, 105, 36, 21],\n",
              " [25, 10, 1334, 90, 4, 247, 105, 36, 21, 374],\n",
              " [25, 10, 1334, 90, 4, 247, 105, 36, 21, 374, 25],\n",
              " [25, 10, 1334, 90, 4, 247, 105, 36, 21, 374, 25, 53],\n",
              " [25, 10, 1334, 90, 4, 247, 105, 36, 21, 374, 25, 53, 497],\n",
              " [1335, 11],\n",
              " [1335, 11, 1],\n",
              " [1335, 11, 1, 367],\n",
              " [1335, 11, 1, 367, 4],\n",
              " [1335, 11, 1, 367, 4, 785],\n",
              " [1335, 11, 1, 367, 4, 785, 2],\n",
              " [1335, 11, 1, 367, 4, 785, 2, 7],\n",
              " [1335, 11, 1, 367, 4, 785, 2, 7, 1336],\n",
              " [1335, 11, 1, 367, 4, 785, 2, 7, 1336, 8],\n",
              " [1, 1337],\n",
              " [1, 1337, 1338],\n",
              " [1, 1337, 1338, 786],\n",
              " [1, 1337, 1338, 786, 1339],\n",
              " [1, 1337, 1338, 786, 1339, 787],\n",
              " [1, 1337, 1338, 786, 1339, 787, 1340],\n",
              " [1, 1337, 1338, 786, 1339, 787, 1340, 291],\n",
              " [1, 1337, 1338, 786, 1339, 787, 1340, 291, 14],\n",
              " [1, 1337, 1338, 786, 1339, 787, 1340, 291, 14, 1341],\n",
              " [3, 25],\n",
              " [3, 25, 10],\n",
              " [3, 25, 10, 1],\n",
              " [3, 25, 10, 1, 375],\n",
              " [3, 25, 10, 1, 375, 6],\n",
              " [3, 25, 10, 1, 375, 6, 376],\n",
              " [3, 25, 10, 1, 375, 6, 376, 35],\n",
              " [3, 25, 10, 1, 375, 6, 376, 35, 1342],\n",
              " [3, 25, 10, 1, 375, 6, 376, 35, 1342, 57],\n",
              " [3, 25, 10, 1, 375, 6, 376, 35, 1342, 57, 210],\n",
              " [3, 25, 10, 1, 375, 6, 376, 35, 1342, 57, 210, 6],\n",
              " [175, 2],\n",
              " [175, 2, 1],\n",
              " [175, 2, 1, 35],\n",
              " [175, 2, 1, 35, 1343],\n",
              " [175, 2, 1, 35, 1343, 1344],\n",
              " [175, 2, 1, 35, 1343, 1344, 93],\n",
              " [175, 2, 1, 35, 1343, 1344, 93, 785],\n",
              " [175, 2, 1, 35, 1343, 1344, 93, 785, 3],\n",
              " [175, 2, 1, 35, 1343, 1344, 93, 785, 3, 1345],\n",
              " [35, 200],\n",
              " [35, 200, 10],\n",
              " [35, 200, 10, 1346],\n",
              " [35, 200, 10, 1346, 161],\n",
              " [35, 200, 10, 1346, 161, 8],\n",
              " [35, 200, 10, 1346, 161, 8, 1],\n",
              " [35, 200, 10, 1346, 161, 8, 1, 108],\n",
              " [35, 200, 10, 1346, 161, 8, 1, 108, 2],\n",
              " [35, 200, 10, 1346, 161, 8, 1, 108, 2, 1347],\n",
              " [287, 1348],\n",
              " [287, 1348, 14],\n",
              " [287, 1348, 14, 1349],\n",
              " [287, 1348, 14, 1349, 49],\n",
              " [287, 1348, 14, 1349, 49, 1],\n",
              " [287, 1348, 14, 1349, 49, 1, 1350],\n",
              " [287, 1348, 14, 1349, 49, 1, 1350, 788],\n",
              " [287, 1348, 14, 1349, 49, 1, 1350, 788, 1351],\n",
              " [287, 1348, 14, 1349, 49, 1, 1350, 788, 1351, 1352],\n",
              " [287, 1348, 14, 1349, 49, 1, 1350, 788, 1351, 1352, 31],\n",
              " [287, 1348, 14, 1349, 49, 1, 1350, 788, 1351, 1352, 31, 144],\n",
              " [287, 1348, 14, 1349, 49, 1, 1350, 788, 1351, 1352, 31, 144, 1353],\n",
              " [287, 1348, 14, 1349, 49, 1, 1350, 788, 1351, 1352, 31, 144, 1353, 7],\n",
              " [287, 1348, 14, 1349, 49, 1, 1350, 788, 1351, 1352, 31, 144, 1353, 7, 1354],\n",
              " [287,\n",
              "  1348,\n",
              "  14,\n",
              "  1349,\n",
              "  49,\n",
              "  1,\n",
              "  1350,\n",
              "  788,\n",
              "  1351,\n",
              "  1352,\n",
              "  31,\n",
              "  144,\n",
              "  1353,\n",
              "  7,\n",
              "  1354,\n",
              "  789],\n",
              " [287,\n",
              "  1348,\n",
              "  14,\n",
              "  1349,\n",
              "  49,\n",
              "  1,\n",
              "  1350,\n",
              "  788,\n",
              "  1351,\n",
              "  1352,\n",
              "  31,\n",
              "  144,\n",
              "  1353,\n",
              "  7,\n",
              "  1354,\n",
              "  789,\n",
              "  4],\n",
              " [287,\n",
              "  1348,\n",
              "  14,\n",
              "  1349,\n",
              "  49,\n",
              "  1,\n",
              "  1350,\n",
              "  788,\n",
              "  1351,\n",
              "  1352,\n",
              "  31,\n",
              "  144,\n",
              "  1353,\n",
              "  7,\n",
              "  1354,\n",
              "  789,\n",
              "  4,\n",
              "  1],\n",
              " [287,\n",
              "  1348,\n",
              "  14,\n",
              "  1349,\n",
              "  49,\n",
              "  1,\n",
              "  1350,\n",
              "  788,\n",
              "  1351,\n",
              "  1352,\n",
              "  31,\n",
              "  144,\n",
              "  1353,\n",
              "  7,\n",
              "  1354,\n",
              "  789,\n",
              "  4,\n",
              "  1,\n",
              "  286],\n",
              " [287,\n",
              "  1348,\n",
              "  14,\n",
              "  1349,\n",
              "  49,\n",
              "  1,\n",
              "  1350,\n",
              "  788,\n",
              "  1351,\n",
              "  1352,\n",
              "  31,\n",
              "  144,\n",
              "  1353,\n",
              "  7,\n",
              "  1354,\n",
              "  789,\n",
              "  4,\n",
              "  1,\n",
              "  286,\n",
              "  2],\n",
              " [287,\n",
              "  1348,\n",
              "  14,\n",
              "  1349,\n",
              "  49,\n",
              "  1,\n",
              "  1350,\n",
              "  788,\n",
              "  1351,\n",
              "  1352,\n",
              "  31,\n",
              "  144,\n",
              "  1353,\n",
              "  7,\n",
              "  1354,\n",
              "  789,\n",
              "  4,\n",
              "  1,\n",
              "  286,\n",
              "  2,\n",
              "  520],\n",
              " [287,\n",
              "  1348,\n",
              "  14,\n",
              "  1349,\n",
              "  49,\n",
              "  1,\n",
              "  1350,\n",
              "  788,\n",
              "  1351,\n",
              "  1352,\n",
              "  31,\n",
              "  144,\n",
              "  1353,\n",
              "  7,\n",
              "  1354,\n",
              "  789,\n",
              "  4,\n",
              "  1,\n",
              "  286,\n",
              "  2,\n",
              "  520,\n",
              "  78],\n",
              " [287,\n",
              "  1348,\n",
              "  14,\n",
              "  1349,\n",
              "  49,\n",
              "  1,\n",
              "  1350,\n",
              "  788,\n",
              "  1351,\n",
              "  1352,\n",
              "  31,\n",
              "  144,\n",
              "  1353,\n",
              "  7,\n",
              "  1354,\n",
              "  789,\n",
              "  4,\n",
              "  1,\n",
              "  286,\n",
              "  2,\n",
              "  520,\n",
              "  78,\n",
              "  24],\n",
              " [287,\n",
              "  1348,\n",
              "  14,\n",
              "  1349,\n",
              "  49,\n",
              "  1,\n",
              "  1350,\n",
              "  788,\n",
              "  1351,\n",
              "  1352,\n",
              "  31,\n",
              "  144,\n",
              "  1353,\n",
              "  7,\n",
              "  1354,\n",
              "  789,\n",
              "  4,\n",
              "  1,\n",
              "  286,\n",
              "  2,\n",
              "  520,\n",
              "  78,\n",
              "  24,\n",
              "  49],\n",
              " [287,\n",
              "  1348,\n",
              "  14,\n",
              "  1349,\n",
              "  49,\n",
              "  1,\n",
              "  1350,\n",
              "  788,\n",
              "  1351,\n",
              "  1352,\n",
              "  31,\n",
              "  144,\n",
              "  1353,\n",
              "  7,\n",
              "  1354,\n",
              "  789,\n",
              "  4,\n",
              "  1,\n",
              "  286,\n",
              "  2,\n",
              "  520,\n",
              "  78,\n",
              "  24,\n",
              "  49,\n",
              "  115],\n",
              " [287,\n",
              "  1348,\n",
              "  14,\n",
              "  1349,\n",
              "  49,\n",
              "  1,\n",
              "  1350,\n",
              "  788,\n",
              "  1351,\n",
              "  1352,\n",
              "  31,\n",
              "  144,\n",
              "  1353,\n",
              "  7,\n",
              "  1354,\n",
              "  789,\n",
              "  4,\n",
              "  1,\n",
              "  286,\n",
              "  2,\n",
              "  520,\n",
              "  78,\n",
              "  24,\n",
              "  49,\n",
              "  115,\n",
              "  287],\n",
              " [287,\n",
              "  1348,\n",
              "  14,\n",
              "  1349,\n",
              "  49,\n",
              "  1,\n",
              "  1350,\n",
              "  788,\n",
              "  1351,\n",
              "  1352,\n",
              "  31,\n",
              "  144,\n",
              "  1353,\n",
              "  7,\n",
              "  1354,\n",
              "  789,\n",
              "  4,\n",
              "  1,\n",
              "  286,\n",
              "  2,\n",
              "  520,\n",
              "  78,\n",
              "  24,\n",
              "  49,\n",
              "  115,\n",
              "  287,\n",
              "  790],\n",
              " [287,\n",
              "  1348,\n",
              "  14,\n",
              "  1349,\n",
              "  49,\n",
              "  1,\n",
              "  1350,\n",
              "  788,\n",
              "  1351,\n",
              "  1352,\n",
              "  31,\n",
              "  144,\n",
              "  1353,\n",
              "  7,\n",
              "  1354,\n",
              "  789,\n",
              "  4,\n",
              "  1,\n",
              "  286,\n",
              "  2,\n",
              "  520,\n",
              "  78,\n",
              "  24,\n",
              "  49,\n",
              "  115,\n",
              "  287,\n",
              "  790,\n",
              "  108],\n",
              " [287,\n",
              "  1348,\n",
              "  14,\n",
              "  1349,\n",
              "  49,\n",
              "  1,\n",
              "  1350,\n",
              "  788,\n",
              "  1351,\n",
              "  1352,\n",
              "  31,\n",
              "  144,\n",
              "  1353,\n",
              "  7,\n",
              "  1354,\n",
              "  789,\n",
              "  4,\n",
              "  1,\n",
              "  286,\n",
              "  2,\n",
              "  520,\n",
              "  78,\n",
              "  24,\n",
              "  49,\n",
              "  115,\n",
              "  287,\n",
              "  790,\n",
              "  108,\n",
              "  4],\n",
              " [287,\n",
              "  1348,\n",
              "  14,\n",
              "  1349,\n",
              "  49,\n",
              "  1,\n",
              "  1350,\n",
              "  788,\n",
              "  1351,\n",
              "  1352,\n",
              "  31,\n",
              "  144,\n",
              "  1353,\n",
              "  7,\n",
              "  1354,\n",
              "  789,\n",
              "  4,\n",
              "  1,\n",
              "  286,\n",
              "  2,\n",
              "  520,\n",
              "  78,\n",
              "  24,\n",
              "  49,\n",
              "  115,\n",
              "  287,\n",
              "  790,\n",
              "  108,\n",
              "  4,\n",
              "  1355],\n",
              " [287,\n",
              "  1348,\n",
              "  14,\n",
              "  1349,\n",
              "  49,\n",
              "  1,\n",
              "  1350,\n",
              "  788,\n",
              "  1351,\n",
              "  1352,\n",
              "  31,\n",
              "  144,\n",
              "  1353,\n",
              "  7,\n",
              "  1354,\n",
              "  789,\n",
              "  4,\n",
              "  1,\n",
              "  286,\n",
              "  2,\n",
              "  520,\n",
              "  78,\n",
              "  24,\n",
              "  49,\n",
              "  115,\n",
              "  287,\n",
              "  790,\n",
              "  108,\n",
              "  4,\n",
              "  1355,\n",
              "  1],\n",
              " [287,\n",
              "  1348,\n",
              "  14,\n",
              "  1349,\n",
              "  49,\n",
              "  1,\n",
              "  1350,\n",
              "  788,\n",
              "  1351,\n",
              "  1352,\n",
              "  31,\n",
              "  144,\n",
              "  1353,\n",
              "  7,\n",
              "  1354,\n",
              "  789,\n",
              "  4,\n",
              "  1,\n",
              "  286,\n",
              "  2,\n",
              "  520,\n",
              "  78,\n",
              "  24,\n",
              "  49,\n",
              "  115,\n",
              "  287,\n",
              "  790,\n",
              "  108,\n",
              "  4,\n",
              "  1355,\n",
              "  1,\n",
              "  791],\n",
              " [377, 10],\n",
              " [377, 10, 9],\n",
              " [377, 10, 9, 143],\n",
              " [377, 10, 9, 143, 145],\n",
              " [377, 10, 9, 143, 145, 53],\n",
              " [377, 10, 9, 143, 145, 53, 28],\n",
              " [377, 10, 9, 143, 145, 53, 28, 1356],\n",
              " [377, 10, 9, 143, 145, 53, 28, 1356, 8],\n",
              " [377, 10, 9, 143, 145, 53, 28, 1356, 8, 521],\n",
              " [377, 10, 9, 143, 145, 53, 28, 1356, 8, 521, 3],\n",
              " [9, 1357],\n",
              " [9, 1357, 200],\n",
              " [9, 1357, 200, 1358],\n",
              " [9, 1357, 200, 1358, 512],\n",
              " [9, 1357, 200, 1358, 512, 1359],\n",
              " [9, 1357, 200, 1358, 512, 1359, 792],\n",
              " [9, 1357, 200, 1358, 512, 1359, 792, 176],\n",
              " [4, 521],\n",
              " [4, 521, 25],\n",
              " [4, 521, 25, 10],\n",
              " [4, 521, 25, 10, 62],\n",
              " [4, 521, 25, 10, 62, 793],\n",
              " [4, 521, 25, 10, 62, 793, 9],\n",
              " [4, 521, 25, 10, 62, 793, 9, 1],\n",
              " [4, 521, 25, 10, 62, 793, 9, 1, 50],\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max([len(x) for x in input_sequences])"
      ],
      "metadata": {
        "id": "nx86tzQ-CYne"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_input_sequences = pad_sequences(input_sequences, maxlen = max_len, padding='pre')"
      ],
      "metadata": {
        "id": "inSUZNy_DCTg"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYW3rbhWDCWr",
        "outputId": "4986c199-0eb5-40a2-a5b4-c2752d92f2cd"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0,   1, 286],\n",
              "       [  0,   0,   0, ...,   1, 286,   2],\n",
              "       [  0,   0,   0, ..., 286,   2,   5],\n",
              "       ...,\n",
              "       [  0,   0,   0, ..., 699, 700, 471],\n",
              "       [  0,   0,   0, ..., 700, 471, 701],\n",
              "       [  0,   0,   0, ..., 471, 701, 702]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = padded_input_sequences[:,:-1]"
      ],
      "metadata": {
        "id": "d7IkbUCuDCaA"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = padded_input_sequences[:,-1]"
      ],
      "metadata": {
        "id": "MIuKAwOlDCcv"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qljCD9JQDCfe",
        "outputId": "44d48c5d-480e-4d18-b326-5c3b5410eb55"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9591, 34)"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDVns6aUDCif",
        "outputId": "fe1e4621-db7a-41c5-972a-e47643c8708a"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0,   0,   1],\n",
              "       [  0,   0,   0, ...,   0,   1, 286],\n",
              "       [  0,   0,   0, ...,   1, 286,   2],\n",
              "       ...,\n",
              "       [  0,   0,   0, ..., 102, 699, 700],\n",
              "       [  0,   0,   0, ..., 699, 700, 471],\n",
              "       [  0,   0,   0, ..., 700, 471, 701]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sHwkB45DCl_",
        "outputId": "fbec57db-e144-46b7-9fdc-318cbf8593a6"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9591,)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qh6FkEb1DOCc",
        "outputId": "14a565e4-2db3-497e-9f6c-68629ae7e290"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([286,   2,   5, ..., 471, 701, 702], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y = to_categorical(y,num_classes=9592)"
      ],
      "metadata": {
        "id": "u5uZRsDmDOGq"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt9-C5qRDOIz",
        "outputId": "daa63369-bda0-4906-9593-448c6683f893"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87k46hB_DOLV",
        "outputId": "e0cfd573-aa91-4ead-bd93-61424ea19a8b"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9591, 9592)"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Using LSTM**"
      ],
      "metadata": {
        "id": "uMT2ZmedTr72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "5nguveQBDOOo"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(9592, 100, input_length=34))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(9592, activation='softmax'))"
      ],
      "metadata": {
        "id": "oMsnO7YJDOSG"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "uzrh9mvTDXE0"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,y,epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8cT86wxDXHn",
        "outputId": "27b8984a-2f0a-4753-fb08-84767ccdcb06"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "300/300 [==============================] - 13s 35ms/step - loss: 7.3694 - accuracy: 0.0461\n",
            "Epoch 2/200\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 6.5359 - accuracy: 0.0653\n",
            "Epoch 3/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 6.3580 - accuracy: 0.0759\n",
            "Epoch 4/200\n",
            "300/300 [==============================] - 3s 8ms/step - loss: 6.1982 - accuracy: 0.0831\n",
            "Epoch 5/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 6.0224 - accuracy: 0.0935\n",
            "Epoch 6/200\n",
            "300/300 [==============================] - 3s 11ms/step - loss: 5.8411 - accuracy: 0.1080\n",
            "Epoch 7/200\n",
            "300/300 [==============================] - 3s 8ms/step - loss: 5.6438 - accuracy: 0.1203\n",
            "Epoch 8/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 5.4471 - accuracy: 0.1343\n",
            "Epoch 9/200\n",
            "300/300 [==============================] - 3s 8ms/step - loss: 5.2568 - accuracy: 0.1394\n",
            "Epoch 10/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 5.0672 - accuracy: 0.1501\n",
            "Epoch 11/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 4.8767 - accuracy: 0.1624\n",
            "Epoch 12/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 4.6846 - accuracy: 0.1765\n",
            "Epoch 13/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 4.4909 - accuracy: 0.1954\n",
            "Epoch 14/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 4.3025 - accuracy: 0.2095\n",
            "Epoch 15/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 4.1084 - accuracy: 0.2322\n",
            "Epoch 16/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 3.9202 - accuracy: 0.2490\n",
            "Epoch 17/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 3.7298 - accuracy: 0.2776\n",
            "Epoch 18/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 3.5395 - accuracy: 0.3037\n",
            "Epoch 19/200\n",
            "300/300 [==============================] - 3s 8ms/step - loss: 3.3513 - accuracy: 0.3350\n",
            "Epoch 20/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 3.1628 - accuracy: 0.3710\n",
            "Epoch 21/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 2.9842 - accuracy: 0.4041\n",
            "Epoch 22/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 2.8061 - accuracy: 0.4373\n",
            "Epoch 23/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 2.6334 - accuracy: 0.4748\n",
            "Epoch 24/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 2.4681 - accuracy: 0.5055\n",
            "Epoch 25/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 2.3128 - accuracy: 0.5347\n",
            "Epoch 26/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 2.1664 - accuracy: 0.5656\n",
            "Epoch 27/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 2.0273 - accuracy: 0.5919\n",
            "Epoch 28/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 1.8994 - accuracy: 0.6187\n",
            "Epoch 29/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 1.7803 - accuracy: 0.6460\n",
            "Epoch 30/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 1.6756 - accuracy: 0.6674\n",
            "Epoch 31/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 1.5671 - accuracy: 0.6899\n",
            "Epoch 32/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 1.4697 - accuracy: 0.7098\n",
            "Epoch 33/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 1.3790 - accuracy: 0.7280\n",
            "Epoch 34/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 1.2921 - accuracy: 0.7453\n",
            "Epoch 35/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 1.2151 - accuracy: 0.7603\n",
            "Epoch 36/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 1.1398 - accuracy: 0.7798\n",
            "Epoch 37/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 1.0717 - accuracy: 0.7901\n",
            "Epoch 38/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 1.0070 - accuracy: 0.8044\n",
            "Epoch 39/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.9470 - accuracy: 0.8196\n",
            "Epoch 40/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.8895 - accuracy: 0.8313\n",
            "Epoch 41/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.8371 - accuracy: 0.8383\n",
            "Epoch 42/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.7877 - accuracy: 0.8503\n",
            "Epoch 43/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.7417 - accuracy: 0.8573\n",
            "Epoch 44/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.6964 - accuracy: 0.8670\n",
            "Epoch 45/200\n",
            "300/300 [==============================] - 3s 8ms/step - loss: 0.6553 - accuracy: 0.8772\n",
            "Epoch 46/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.6195 - accuracy: 0.8829\n",
            "Epoch 47/200\n",
            "300/300 [==============================] - 3s 8ms/step - loss: 0.5842 - accuracy: 0.8911\n",
            "Epoch 48/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.5501 - accuracy: 0.8973\n",
            "Epoch 49/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.5187 - accuracy: 0.9030\n",
            "Epoch 50/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.4923 - accuracy: 0.9071\n",
            "Epoch 51/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.4650 - accuracy: 0.9130\n",
            "Epoch 52/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.4394 - accuracy: 0.9167\n",
            "Epoch 53/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.4176 - accuracy: 0.9193\n",
            "Epoch 54/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.3959 - accuracy: 0.9214\n",
            "Epoch 55/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.3761 - accuracy: 0.9284\n",
            "Epoch 56/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.3569 - accuracy: 0.9291\n",
            "Epoch 57/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.3385 - accuracy: 0.9319\n",
            "Epoch 58/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.3233 - accuracy: 0.9342\n",
            "Epoch 59/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.3078 - accuracy: 0.9347\n",
            "Epoch 60/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.2940 - accuracy: 0.9373\n",
            "Epoch 61/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.2808 - accuracy: 0.9392\n",
            "Epoch 62/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.2702 - accuracy: 0.9390\n",
            "Epoch 63/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.2590 - accuracy: 0.9413\n",
            "Epoch 64/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.2510 - accuracy: 0.9413\n",
            "Epoch 65/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.2411 - accuracy: 0.9440\n",
            "Epoch 66/200\n",
            "300/300 [==============================] - 3s 8ms/step - loss: 0.2324 - accuracy: 0.9439\n",
            "Epoch 67/200\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.2252 - accuracy: 0.9453\n",
            "Epoch 68/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.2177 - accuracy: 0.9441\n",
            "Epoch 69/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.2120 - accuracy: 0.9426\n",
            "Epoch 70/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.2050 - accuracy: 0.9460\n",
            "Epoch 71/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.2003 - accuracy: 0.9457\n",
            "Epoch 72/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1951 - accuracy: 0.9470\n",
            "Epoch 73/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1900 - accuracy: 0.9460\n",
            "Epoch 74/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1862 - accuracy: 0.9477\n",
            "Epoch 75/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1850 - accuracy: 0.9463\n",
            "Epoch 76/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.1792 - accuracy: 0.9468\n",
            "Epoch 77/200\n",
            "300/300 [==============================] - 3s 8ms/step - loss: 0.1768 - accuracy: 0.9465\n",
            "Epoch 78/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1728 - accuracy: 0.9481\n",
            "Epoch 79/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1699 - accuracy: 0.9475\n",
            "Epoch 80/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1669 - accuracy: 0.9468\n",
            "Epoch 81/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.1663 - accuracy: 0.9465\n",
            "Epoch 82/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.1622 - accuracy: 0.9472\n",
            "Epoch 83/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1603 - accuracy: 0.9479\n",
            "Epoch 84/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1591 - accuracy: 0.9465\n",
            "Epoch 85/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1574 - accuracy: 0.9457\n",
            "Epoch 86/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1561 - accuracy: 0.9470\n",
            "Epoch 87/200\n",
            "300/300 [==============================] - 3s 11ms/step - loss: 0.1554 - accuracy: 0.9476\n",
            "Epoch 88/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1539 - accuracy: 0.9476\n",
            "Epoch 89/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1526 - accuracy: 0.9483\n",
            "Epoch 90/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1498 - accuracy: 0.9472\n",
            "Epoch 91/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1486 - accuracy: 0.9481\n",
            "Epoch 92/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.1484 - accuracy: 0.9483\n",
            "Epoch 93/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.1474 - accuracy: 0.9483\n",
            "Epoch 94/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1461 - accuracy: 0.9476\n",
            "Epoch 95/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1461 - accuracy: 0.9475\n",
            "Epoch 96/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1461 - accuracy: 0.9478\n",
            "Epoch 97/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1452 - accuracy: 0.9477\n",
            "Epoch 98/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.1431 - accuracy: 0.9476\n",
            "Epoch 99/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1426 - accuracy: 0.9470\n",
            "Epoch 100/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1421 - accuracy: 0.9476\n",
            "Epoch 101/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1422 - accuracy: 0.9487\n",
            "Epoch 102/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1467 - accuracy: 0.9473\n",
            "Epoch 103/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.1402 - accuracy: 0.9485\n",
            "Epoch 104/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.1387 - accuracy: 0.9481\n",
            "Epoch 105/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1382 - accuracy: 0.9480\n",
            "Epoch 106/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1383 - accuracy: 0.9468\n",
            "Epoch 107/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1380 - accuracy: 0.9487\n",
            "Epoch 108/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.1375 - accuracy: 0.9489\n",
            "Epoch 109/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.1378 - accuracy: 0.9483\n",
            "Epoch 110/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1380 - accuracy: 0.9490\n",
            "Epoch 111/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1374 - accuracy: 0.9472\n",
            "Epoch 112/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1364 - accuracy: 0.9488\n",
            "Epoch 113/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1362 - accuracy: 0.9482\n",
            "Epoch 114/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.1352 - accuracy: 0.9478\n",
            "Epoch 115/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1351 - accuracy: 0.9490\n",
            "Epoch 116/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1356 - accuracy: 0.9482\n",
            "Epoch 117/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1355 - accuracy: 0.9480\n",
            "Epoch 118/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1345 - accuracy: 0.9483\n",
            "Epoch 119/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.1342 - accuracy: 0.9477\n",
            "Epoch 120/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.1334 - accuracy: 0.9493\n",
            "Epoch 121/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1334 - accuracy: 0.9478\n",
            "Epoch 122/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1334 - accuracy: 0.9473\n",
            "Epoch 123/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1331 - accuracy: 0.9484\n",
            "Epoch 124/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.1335 - accuracy: 0.9473\n",
            "Epoch 125/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.1336 - accuracy: 0.9487\n",
            "Epoch 126/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1360 - accuracy: 0.9467\n",
            "Epoch 127/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1345 - accuracy: 0.9482\n",
            "Epoch 128/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1328 - accuracy: 0.9482\n",
            "Epoch 129/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1318 - accuracy: 0.9487\n",
            "Epoch 130/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.1317 - accuracy: 0.9482\n",
            "Epoch 131/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.1314 - accuracy: 0.9479\n",
            "Epoch 132/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1309 - accuracy: 0.9480\n",
            "Epoch 133/200\n",
            "300/300 [==============================] - 3s 8ms/step - loss: 0.1310 - accuracy: 0.9489\n",
            "Epoch 134/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.1310 - accuracy: 0.9491\n",
            "Epoch 135/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.1313 - accuracy: 0.9482\n",
            "Epoch 136/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.1310 - accuracy: 0.9487\n",
            "Epoch 137/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1307 - accuracy: 0.9485\n",
            "Epoch 138/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1306 - accuracy: 0.9482\n",
            "Epoch 139/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1308 - accuracy: 0.9486\n",
            "Epoch 140/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.1309 - accuracy: 0.9488\n",
            "Epoch 141/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.1337 - accuracy: 0.9471\n",
            "Epoch 142/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1424 - accuracy: 0.9458\n",
            "Epoch 143/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1436 - accuracy: 0.9466\n",
            "Epoch 144/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1325 - accuracy: 0.9488\n",
            "Epoch 145/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.1309 - accuracy: 0.9478\n",
            "Epoch 146/200\n",
            "300/300 [==============================] - 3s 11ms/step - loss: 0.1292 - accuracy: 0.9500\n",
            "Epoch 147/200\n",
            "300/300 [==============================] - 3s 8ms/step - loss: 0.1291 - accuracy: 0.9493\n",
            "Epoch 148/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1292 - accuracy: 0.9482\n",
            "Epoch 149/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1287 - accuracy: 0.9480\n",
            "Epoch 150/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.1289 - accuracy: 0.9491\n",
            "Epoch 151/200\n",
            "300/300 [==============================] - 3s 11ms/step - loss: 0.1290 - accuracy: 0.9487\n",
            "Epoch 152/200\n",
            "300/300 [==============================] - 3s 8ms/step - loss: 0.1294 - accuracy: 0.9483\n",
            "Epoch 153/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1297 - accuracy: 0.9492\n",
            "Epoch 154/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1292 - accuracy: 0.9475\n",
            "Epoch 155/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1297 - accuracy: 0.9473\n",
            "Epoch 156/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.1292 - accuracy: 0.9476\n",
            "Epoch 157/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.1290 - accuracy: 0.9482\n",
            "Epoch 158/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1294 - accuracy: 0.9485\n",
            "Epoch 159/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1291 - accuracy: 0.9479\n",
            "Epoch 160/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1292 - accuracy: 0.9483\n",
            "Epoch 161/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.1285 - accuracy: 0.9480\n",
            "Epoch 162/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.1285 - accuracy: 0.9497\n",
            "Epoch 163/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1285 - accuracy: 0.9480\n",
            "Epoch 164/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1284 - accuracy: 0.9492\n",
            "Epoch 165/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1294 - accuracy: 0.9476\n",
            "Epoch 166/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1326 - accuracy: 0.9482\n",
            "Epoch 167/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.1427 - accuracy: 0.9455\n",
            "Epoch 168/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.1317 - accuracy: 0.9475\n",
            "Epoch 169/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1283 - accuracy: 0.9490\n",
            "Epoch 170/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1278 - accuracy: 0.9487\n",
            "Epoch 171/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1278 - accuracy: 0.9491\n",
            "Epoch 172/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.1279 - accuracy: 0.9487\n",
            "Epoch 173/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.1274 - accuracy: 0.9485\n",
            "Epoch 174/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1279 - accuracy: 0.9485\n",
            "Epoch 175/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1278 - accuracy: 0.9494\n",
            "Epoch 176/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1275 - accuracy: 0.9487\n",
            "Epoch 177/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1271 - accuracy: 0.9480\n",
            "Epoch 178/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.1275 - accuracy: 0.9494\n",
            "Epoch 179/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1276 - accuracy: 0.9472\n",
            "Epoch 180/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1279 - accuracy: 0.9480\n",
            "Epoch 181/200\n",
            "300/300 [==============================] - 3s 8ms/step - loss: 0.1272 - accuracy: 0.9487\n",
            "Epoch 182/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1280 - accuracy: 0.9477\n",
            "Epoch 183/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.1273 - accuracy: 0.9486\n",
            "Epoch 184/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.1276 - accuracy: 0.9484\n",
            "Epoch 185/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1276 - accuracy: 0.9490\n",
            "Epoch 186/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1277 - accuracy: 0.9495\n",
            "Epoch 187/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1387 - accuracy: 0.9462\n",
            "Epoch 188/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.1411 - accuracy: 0.9464\n",
            "Epoch 189/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.1302 - accuracy: 0.9476\n",
            "Epoch 190/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1271 - accuracy: 0.9478\n",
            "Epoch 191/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1266 - accuracy: 0.9488\n",
            "Epoch 192/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1259 - accuracy: 0.9500\n",
            "Epoch 193/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1268 - accuracy: 0.9486\n",
            "Epoch 194/200\n",
            "300/300 [==============================] - 3s 11ms/step - loss: 0.1267 - accuracy: 0.9492\n",
            "Epoch 195/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1261 - accuracy: 0.9491\n",
            "Epoch 196/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.1268 - accuracy: 0.9482\n",
            "Epoch 197/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1267 - accuracy: 0.9481\n",
            "Epoch 198/200\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.1268 - accuracy: 0.9490\n",
            "Epoch 199/200\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.1264 - accuracy: 0.9495\n",
            "Epoch 200/200\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.1267 - accuracy: 0.9487\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e69cc0b41c0>"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "cX1jEPFsDXKc"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "text = \"β - cell failure\"\n",
        "\n",
        "for i in range(20):\n",
        "  # tokenize\n",
        "  token_text = tokenizer.texts_to_sequences([text])[0]\n",
        "  # padding\n",
        "  padded_token_text = pad_sequences([token_text], maxlen=34, padding='pre')\n",
        "  # predict\n",
        "  pos = np.argmax(model.predict(padded_token_text))\n",
        "\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if index == pos:\n",
        "      text = text + \" \" + word\n",
        "      print(text)\n",
        "      time.sleep(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2Nb3sPpDXNL",
        "outputId": "fbfba999-07d5-40be-a927-481545755f07"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 30ms/step\n",
            "β - cell failure the\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "β - cell failure the same\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "β - cell failure the same with\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "β - cell failure the same with collip\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "β - cell failure the same with collip and\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "β - cell failure the same with collip and the\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "β - cell failure the same with collip and the diagnosis\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "β - cell failure the same with collip and the diagnosis was\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "β - cell failure the same with collip and the diagnosis was superseded\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "β - cell failure the same with collip and the diagnosis was superseded by\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "β - cell failure the same with collip and the diagnosis was superseded by chemical\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "β - cell failure the same with collip and the diagnosis was superseded by chemical tests\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "β - cell failure the same with collip and the diagnosis was superseded by chemical tests for\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "β - cell failure the same with collip and the diagnosis was superseded by chemical tests for reducing\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "β - cell failure the same with collip and the diagnosis was superseded by chemical tests for reducing agents\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "β - cell failure the same with collip and the diagnosis was superseded by chemical tests for reducing agents such\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "β - cell failure the same with collip and the diagnosis was superseded by chemical tests for reducing agents such as\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "β - cell failure the same with collip and the diagnosis was superseded by chemical tests for reducing agents such as glucose\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "β - cell failure the same with collip and the diagnosis was superseded by chemical tests for reducing agents such as glucose as\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "β - cell failure the same with collip and the diagnosis was superseded by chemical tests for reducing agents such as glucose as 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bidirectional GRU**"
      ],
      "metadata": {
        "id": "_nNNwN7DT6NQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Bidirectional, GRU, Dense, Embedding"
      ],
      "metadata": {
        "id": "VZwsdQIOT1SM"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Add an embedding layer\n",
        "model.add(Embedding(input_dim=9592, output_dim=100, input_length=34))\n",
        "\n",
        "# Add first bidirectional GRU layer\n",
        "model.add(Bidirectional(GRU(units=150, return_sequences=True)))\n",
        "\n",
        "# Add second bidirectional GRU layer\n",
        "model.add(Bidirectional(GRU(units=150)))\n",
        "\n",
        "# Add a dense layer with softmax activation for output\n",
        "model.add(Dense(9592, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOT19O8zDXQu",
        "outputId": "0b2d6d0a-2d49-494c-e35c-1466254759eb"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 34, 100)           959200    \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 34, 300)           226800    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 300)               406800    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 9592)              2887192   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4479992 (17.09 MB)\n",
            "Trainable params: 4479992 (17.09 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,y,epochs=150)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UR0HmBMDXTO",
        "outputId": "5bdeb1bd-9148-4e92-9392-0d99cf2e322f"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "300/300 [==============================] - 5s 18ms/step - loss: 0.2104 - accuracy: 0.9322\n",
            "Epoch 2/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1727 - accuracy: 0.9432\n",
            "Epoch 3/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1599 - accuracy: 0.9462\n",
            "Epoch 4/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1561 - accuracy: 0.9464\n",
            "Epoch 5/150\n",
            "300/300 [==============================] - 7s 24ms/step - loss: 0.1546 - accuracy: 0.9479\n",
            "Epoch 6/150\n",
            "300/300 [==============================] - 9s 29ms/step - loss: 0.1547 - accuracy: 0.9466\n",
            "Epoch 7/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1531 - accuracy: 0.9472\n",
            "Epoch 8/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1532 - accuracy: 0.9478\n",
            "Epoch 9/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1550 - accuracy: 0.9457\n",
            "Epoch 10/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1548 - accuracy: 0.9480\n",
            "Epoch 11/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1594 - accuracy: 0.9472\n",
            "Epoch 12/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1577 - accuracy: 0.9471\n",
            "Epoch 13/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1565 - accuracy: 0.9467\n",
            "Epoch 14/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1604 - accuracy: 0.9460\n",
            "Epoch 15/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1640 - accuracy: 0.9453\n",
            "Epoch 16/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1606 - accuracy: 0.9469\n",
            "Epoch 17/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1624 - accuracy: 0.9461\n",
            "Epoch 18/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1561 - accuracy: 0.9469\n",
            "Epoch 19/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1551 - accuracy: 0.9481\n",
            "Epoch 20/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1573 - accuracy: 0.9472\n",
            "Epoch 21/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1540 - accuracy: 0.9466\n",
            "Epoch 22/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1545 - accuracy: 0.9472\n",
            "Epoch 23/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1536 - accuracy: 0.9465\n",
            "Epoch 24/150\n",
            "300/300 [==============================] - 5s 18ms/step - loss: 0.1547 - accuracy: 0.9480\n",
            "Epoch 25/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1548 - accuracy: 0.9464\n",
            "Epoch 26/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1605 - accuracy: 0.9478\n",
            "Epoch 27/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1700 - accuracy: 0.9436\n",
            "Epoch 28/150\n",
            "300/300 [==============================] - 4s 15ms/step - loss: 0.2077 - accuracy: 0.9316\n",
            "Epoch 29/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1825 - accuracy: 0.9395\n",
            "Epoch 30/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1630 - accuracy: 0.9452\n",
            "Epoch 31/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1518 - accuracy: 0.9472\n",
            "Epoch 32/150\n",
            "300/300 [==============================] - 4s 15ms/step - loss: 0.1528 - accuracy: 0.9461\n",
            "Epoch 33/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1499 - accuracy: 0.9477\n",
            "Epoch 34/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1524 - accuracy: 0.9477\n",
            "Epoch 35/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1523 - accuracy: 0.9466\n",
            "Epoch 36/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1528 - accuracy: 0.9476\n",
            "Epoch 37/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1509 - accuracy: 0.9485\n",
            "Epoch 38/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1516 - accuracy: 0.9464\n",
            "Epoch 39/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.2272 - accuracy: 0.9320\n",
            "Epoch 40/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.2058 - accuracy: 0.9333\n",
            "Epoch 41/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1639 - accuracy: 0.9462\n",
            "Epoch 42/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1528 - accuracy: 0.9475\n",
            "Epoch 43/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1527 - accuracy: 0.9473\n",
            "Epoch 44/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1518 - accuracy: 0.9486\n",
            "Epoch 45/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1515 - accuracy: 0.9465\n",
            "Epoch 46/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1504 - accuracy: 0.9472\n",
            "Epoch 47/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1489 - accuracy: 0.9472\n",
            "Epoch 48/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1525 - accuracy: 0.9463\n",
            "Epoch 49/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1513 - accuracy: 0.9461\n",
            "Epoch 50/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1502 - accuracy: 0.9472\n",
            "Epoch 51/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1533 - accuracy: 0.9477\n",
            "Epoch 52/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1580 - accuracy: 0.9469\n",
            "Epoch 53/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1815 - accuracy: 0.9398\n",
            "Epoch 54/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1757 - accuracy: 0.9433\n",
            "Epoch 55/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1601 - accuracy: 0.9468\n",
            "Epoch 56/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1532 - accuracy: 0.9475\n",
            "Epoch 57/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1503 - accuracy: 0.9484\n",
            "Epoch 58/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1517 - accuracy: 0.9460\n",
            "Epoch 59/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1486 - accuracy: 0.9468\n",
            "Epoch 60/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1509 - accuracy: 0.9463\n",
            "Epoch 61/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1491 - accuracy: 0.9475\n",
            "Epoch 62/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1497 - accuracy: 0.9477\n",
            "Epoch 63/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1501 - accuracy: 0.9469\n",
            "Epoch 64/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1485 - accuracy: 0.9483\n",
            "Epoch 65/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1556 - accuracy: 0.9462\n",
            "Epoch 66/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1591 - accuracy: 0.9462\n",
            "Epoch 67/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1801 - accuracy: 0.9409\n",
            "Epoch 68/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1946 - accuracy: 0.9358\n",
            "Epoch 69/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1663 - accuracy: 0.9424\n",
            "Epoch 70/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1539 - accuracy: 0.9476\n",
            "Epoch 71/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1494 - accuracy: 0.9483\n",
            "Epoch 72/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1478 - accuracy: 0.9470\n",
            "Epoch 73/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1480 - accuracy: 0.9461\n",
            "Epoch 74/150\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.1475 - accuracy: 0.9481\n",
            "Epoch 75/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1475 - accuracy: 0.9475\n",
            "Epoch 76/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1489 - accuracy: 0.9465\n",
            "Epoch 77/150\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.1497 - accuracy: 0.9476\n",
            "Epoch 78/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1498 - accuracy: 0.9478\n",
            "Epoch 79/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1524 - accuracy: 0.9480\n",
            "Epoch 80/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1524 - accuracy: 0.9465\n",
            "Epoch 81/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1548 - accuracy: 0.9472\n",
            "Epoch 82/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1594 - accuracy: 0.9449\n",
            "Epoch 83/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1656 - accuracy: 0.9436\n",
            "Epoch 84/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1643 - accuracy: 0.9445\n",
            "Epoch 85/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1558 - accuracy: 0.9458\n",
            "Epoch 86/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1511 - accuracy: 0.9467\n",
            "Epoch 87/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1481 - accuracy: 0.9468\n",
            "Epoch 88/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1466 - accuracy: 0.9472\n",
            "Epoch 89/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1456 - accuracy: 0.9476\n",
            "Epoch 90/150\n",
            "300/300 [==============================] - 5s 16ms/step - loss: 0.1460 - accuracy: 0.9478\n",
            "Epoch 91/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1479 - accuracy: 0.9466\n",
            "Epoch 92/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1489 - accuracy: 0.9471\n",
            "Epoch 93/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1487 - accuracy: 0.9476\n",
            "Epoch 94/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1483 - accuracy: 0.9479\n",
            "Epoch 95/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1535 - accuracy: 0.9469\n",
            "Epoch 96/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1699 - accuracy: 0.9418\n",
            "Epoch 97/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1962 - accuracy: 0.9335\n",
            "Epoch 98/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1753 - accuracy: 0.9422\n",
            "Epoch 99/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1559 - accuracy: 0.9434\n",
            "Epoch 100/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1468 - accuracy: 0.9483\n",
            "Epoch 101/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1457 - accuracy: 0.9477\n",
            "Epoch 102/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1463 - accuracy: 0.9468\n",
            "Epoch 103/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1457 - accuracy: 0.9473\n",
            "Epoch 104/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1455 - accuracy: 0.9468\n",
            "Epoch 105/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1453 - accuracy: 0.9467\n",
            "Epoch 106/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1463 - accuracy: 0.9471\n",
            "Epoch 107/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1480 - accuracy: 0.9462\n",
            "Epoch 108/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1472 - accuracy: 0.9470\n",
            "Epoch 109/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1466 - accuracy: 0.9463\n",
            "Epoch 110/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1452 - accuracy: 0.9481\n",
            "Epoch 111/150\n",
            "300/300 [==============================] - 6s 19ms/step - loss: 0.1518 - accuracy: 0.9472\n",
            "Epoch 112/150\n",
            "300/300 [==============================] - 5s 16ms/step - loss: 0.1634 - accuracy: 0.9436\n",
            "Epoch 113/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1907 - accuracy: 0.9365\n",
            "Epoch 114/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1745 - accuracy: 0.9405\n",
            "Epoch 115/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1554 - accuracy: 0.9448\n",
            "Epoch 116/150\n",
            "300/300 [==============================] - 4s 15ms/step - loss: 0.1467 - accuracy: 0.9479\n",
            "Epoch 117/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1452 - accuracy: 0.9475\n",
            "Epoch 118/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1438 - accuracy: 0.9470\n",
            "Epoch 119/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1434 - accuracy: 0.9472\n",
            "Epoch 120/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1428 - accuracy: 0.9477\n",
            "Epoch 121/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1443 - accuracy: 0.9459\n",
            "Epoch 122/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1458 - accuracy: 0.9475\n",
            "Epoch 123/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1486 - accuracy: 0.9471\n",
            "Epoch 124/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1470 - accuracy: 0.9470\n",
            "Epoch 125/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1465 - accuracy: 0.9480\n",
            "Epoch 126/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1469 - accuracy: 0.9464\n",
            "Epoch 127/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1447 - accuracy: 0.9489\n",
            "Epoch 128/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1526 - accuracy: 0.9468\n",
            "Epoch 129/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1773 - accuracy: 0.9397\n",
            "Epoch 130/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1771 - accuracy: 0.9405\n",
            "Epoch 131/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1534 - accuracy: 0.9447\n",
            "Epoch 132/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1447 - accuracy: 0.9482\n",
            "Epoch 133/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1425 - accuracy: 0.9486\n",
            "Epoch 134/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1418 - accuracy: 0.9493\n",
            "Epoch 135/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1435 - accuracy: 0.9478\n",
            "Epoch 136/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1420 - accuracy: 0.9483\n",
            "Epoch 137/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1461 - accuracy: 0.9473\n",
            "Epoch 138/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1451 - accuracy: 0.9469\n",
            "Epoch 139/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1489 - accuracy: 0.9470\n",
            "Epoch 140/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1465 - accuracy: 0.9480\n",
            "Epoch 141/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1450 - accuracy: 0.9473\n",
            "Epoch 142/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1462 - accuracy: 0.9466\n",
            "Epoch 143/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1485 - accuracy: 0.9466\n",
            "Epoch 144/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1595 - accuracy: 0.9447\n",
            "Epoch 145/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1898 - accuracy: 0.9347\n",
            "Epoch 146/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1749 - accuracy: 0.9388\n",
            "Epoch 147/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1571 - accuracy: 0.9442\n",
            "Epoch 148/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1481 - accuracy: 0.9461\n",
            "Epoch 149/150\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.1449 - accuracy: 0.9479\n",
            "Epoch 150/150\n",
            "300/300 [==============================] - 5s 17ms/step - loss: 0.1436 - accuracy: 0.9471\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e69ccf157b0>"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "text = \"Polyuric diseases have been described\"\n",
        "\n",
        "for i in range(15):\n",
        "  # tokenize\n",
        "  token_text = tokenizer.texts_to_sequences([text])[0]\n",
        "  # padding\n",
        "  padded_token_text = pad_sequences([token_text], maxlen=34, padding='pre')\n",
        "  # predict\n",
        "  pos = np.argmax(model.predict(padded_token_text))\n",
        "\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if index == pos:\n",
        "      text = text + \" \" + word\n",
        "      print(text)\n",
        "      time.sleep(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGLo8VxiDXV_",
        "outputId": "65eb0304-bdbf-487f-c717-81b67122722f"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n",
            "Polyuric diseases have been described for\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Polyuric diseases have been described for over\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Polyuric diseases have been described for over 3500\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Polyuric diseases have been described for over 3500 years\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Polyuric diseases have been described for over 3500 years the\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Polyuric diseases have been described for over 3500 years the name\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Polyuric diseases have been described for over 3500 years the name patients\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Polyuric diseases have been described for over 3500 years the name patients with\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Polyuric diseases have been described for over 3500 years the name patients with diabetes\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Polyuric diseases have been described for over 3500 years the name patients with diabetes medications\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Polyuric diseases have been described for over 3500 years the name patients with diabetes medications and\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Polyuric diseases have been described for over 3500 years the name patients with diabetes medications and europe\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Polyuric diseases have been described for over 3500 years the name patients with diabetes medications and europe type\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Polyuric diseases have been described for over 3500 years the name patients with diabetes medications and europe type ii\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Polyuric diseases have been described for over 3500 years the name patients with diabetes medications and europe type ii study\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MV0tv1JgDXbe"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QmCeA9rADXez"
      },
      "execution_count": 113,
      "outputs": []
    }
  ]
}